{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/rendering/network\n",
      "/workspace/rendering\n",
      "Decoder_b0 (214,)\n",
      "Decoder_w0 (328704,)\n",
      "DenseRes0_b0 (1536,)\n",
      "DenseRes0_w0 (2359296,)\n",
      "DenseRes1_b0 (1536,)\n",
      "DenseRes1_w0 (2359296,)\n",
      "DenseRes2_b0 (1536,)\n",
      "DenseRes2_w0 (2359296,)\n",
      "DenseRes3_b0 (1536,)\n",
      "DenseRes3_w0 (2359296,)\n",
      "Encoder0_b0 (512,)\n",
      "Encoder0_w0 (98304,)\n",
      "Encoder1_b0 (512,)\n",
      "Encoder1_w0 (172032,)\n",
      "Encoder2_b0 (512,)\n",
      "Encoder2_w0 (932864,)\n",
      "Xmean (2350,)\n",
      "Xstd (2350,)\n",
      "Ymean (214,)\n",
      "Ystd (214,)\n",
      "/workspace/rendering/data/ManipNetBIN\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "os.chdir(\"..\")\n",
    "print(os.getcwd())\n",
    "\n",
    "bindata = {}\n",
    "for fp in sorted(Path(\"data/ManipNetBIN\").iterdir()):\n",
    "    # print(fp)\n",
    "    bindata[fp.stem] = np.fromfile(fp, dtype=np.float32)\n",
    "for k, v in bindata.items():\n",
    "    print(k, v.shape)\n",
    "\n",
    "os.chdir(\"data/ManipNetBIN\")\n",
    "print(os.getcwd())\n",
    "# num = np.fromfile('Decoder_b0.bin')\n",
    "# print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_vec, out_vec):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.FC0= nn.Linear(in_vec, out_vec)\n",
    "        self.FC1 = nn.Linear(in_vec, out_vec)\n",
    "        # self.relu = nn.ReLU()\n",
    "\n",
    "        # self.sequence = nn.Sequential(\n",
    "        #     [\n",
    "        #         nn.Linear(),\n",
    "        #         nn.ReLU(),\n",
    "        #     ]\n",
    "        # )\n",
    "\n",
    "    def __call__(self, H_zero):\n",
    "        # H_zero = nn.ReLU(H_zero)\n",
    "        H_zero_ = F.relu(H_zero)\n",
    "        H_one = self.FC0(H_zero_) + H_zero_\n",
    "        # H_one  = nn.ReLU(H_one)\n",
    "        H_one_ = F.relu(H_one)\n",
    "        H_two = self.FC1(H_one_) + H_one_ + H_zero_\n",
    "        return H_two\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n",
      "cuda:0\n",
      "NeuralNetwork(\n",
      "  (Encoder0): Linear(in_features=192, out_features=512, bias=True)\n",
      "  (Encoder1): Linear(in_features=336, out_features=512, bias=True)\n",
      "  (Encoder2): Linear(in_features=1822, out_features=512, bias=True)\n",
      "  (DenseRes0): ResBlock(\n",
      "    (FC0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (FC1): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "  )\n",
      "  (DenseRes1): ResBlock(\n",
      "    (FC0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (FC1): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "  )\n",
      "  (Decoder): Linear(in_features=1536, out_features=214, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 訓練に際して、可能であればGPU（cuda）を設定します。GPUが搭載されていない場合はCPUを使用します\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "# modelを定義します\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.Encoder0= nn.Linear(192, 512)\n",
    "        self.Encoder1 = nn.Linear(336, 512)\n",
    "        self.Encoder2 = nn.Linear(1822, 512)\n",
    "        self.DenseRes0 = ResBlock(1536, 1536)\n",
    "        self.DenseRes1 = ResBlock(1536, 1536)\n",
    "        self.Decoder = nn.Linear(1536, 214)\n",
    "\n",
    "    def forward(self, pose, traj, sensor):\n",
    "        pose_ = self.Encoder0(pose)\n",
    "        traj_ = self.Encoder1(traj)\n",
    "        sensor_ = self.Encoder2(sensor)\n",
    "        out = torch.cat([pose_, traj_, sensor_], dim=-1)\n",
    "        out = self.DenseRes0(out)\n",
    "        out = self.DenseRes1(out)\n",
    "        out = self.Decoder(out)\n",
    "        return out\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(device)\n",
    "print(model)\n",
    "# model()\n",
    "# print(model.device)\n",
    "# for param in model.Encoder1.parameters():\n",
    "#     print(\"encoder\", param.shape)\n",
    "pose, traj, sensor = torch.zeros([1,192]).to(device), torch.zeros([1,336]).to(device), torch.zeros([1,1822]).to(device)\n",
    "y = model(pose, traj, sensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder0.weight torch.Size([512, 192])\n",
      "Encoder0.bias torch.Size([512])\n",
      "Encoder1.weight torch.Size([512, 336])\n",
      "Encoder1.bias torch.Size([512])\n",
      "Encoder2.weight torch.Size([512, 1822])\n",
      "Encoder2.bias torch.Size([512])\n",
      "DenseRes0.FC0.weight torch.Size([1536, 1536])\n",
      "DenseRes0.FC0.bias torch.Size([1536])\n",
      "DenseRes0.FC1.weight torch.Size([1536, 1536])\n",
      "DenseRes0.FC1.bias torch.Size([1536])\n",
      "DenseRes1.FC0.weight torch.Size([1536, 1536])\n",
      "DenseRes1.FC0.bias torch.Size([1536])\n",
      "DenseRes1.FC1.weight torch.Size([1536, 1536])\n",
      "DenseRes1.FC1.bias torch.Size([1536])\n",
      "Decoder.weight torch.Size([214, 1536])\n",
      "Decoder.bias torch.Size([214])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder_b0 (214,)\n",
      "Decoder_w0 (328704,)\n",
      "DenseRes0_b0 (1536,)\n",
      "DenseRes0_w0 (2359296,)\n",
      "DenseRes1_b0 (1536,)\n",
      "DenseRes1_w0 (2359296,)\n",
      "DenseRes2_b0 (1536,)\n",
      "DenseRes2_w0 (2359296,)\n",
      "DenseRes3_b0 (1536,)\n",
      "DenseRes3_w0 (2359296,)\n",
      "Encoder0_b0 (512,)\n",
      "Encoder0_w0 (98304,)\n",
      "Encoder1_b0 (512,)\n",
      "Encoder1_w0 (172032,)\n",
      "Encoder2_b0 (512,)\n",
      "Encoder2_w0 (932864,)\n",
      "Xmean (2350,)\n",
      "Xstd (2350,)\n",
      "Ymean (214,)\n",
      "Ystd (214,)\n"
     ]
    }
   ],
   "source": [
    "for k, v in bindata.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Decoder', 'Encoder0', 'Encoder1', 'Encoder2']\n",
      "[['Decoder.weight', 'Decoder_w0', [214, 1536]], ['Decoder.bias', 'Decoder_b0', [214]], ['Encoder0.weight', 'Encoder0_w0', [512, 192]], ['Encoder0.bias', 'Encoder0_b0', [512]], ['Encoder1.weight', 'Encoder1_w0', [512, 336]], ['Encoder1.bias', 'Encoder1_b0', [512]], ['Encoder2.weight', 'Encoder2_w0', [512, 1822]], ['Encoder2.bias', 'Encoder2_b0', [512]]]\n"
     ]
    }
   ],
   "source": [
    "netlist = [\"Decoder\"]\n",
    "net_name_list = []\n",
    "# for i in range(4):\n",
    "#     netlist.append(\"DenseRes\" + str(i))\n",
    "for i in range(3):\n",
    "    netlist.append(\"Encoder\" + str(i))\n",
    "print(netlist)\n",
    "for name in netlist:\n",
    "    net_name_list.append([name + \".weight\", name + \"_w0\"])\n",
    "    net_name_list.append([name + \".bias\", name + \"_b0\"])\n",
    "\n",
    "shape_list = [[214, 1536], [214], [512, 192], [512], [512, 336], [512], [512, 1822], [512]]\n",
    "for i in range(len(net_name_list)):\n",
    "    net_name_list[i].append(shape_list[i])\n",
    "print(net_name_list)\n",
    "\n",
    "dense_weight_list = []\n",
    "dense_bias_list = []\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        dense_weight_list.append([\"DenseRes\" + str(i) + \".FC\" + str(j) + \".weight\", \"DenseRes\" + str(2 * i + j) + \"_w0\"])\n",
    "        dense_bias_list.append([\"DenseRes\" + str(i) + \".FC\" + str(j) + \".bias\", \"DenseRes\" + str(2 * i + j) + \"_b0\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder0.weight Encoder0_w0\n",
      "(98304,)\n",
      "torch.Size([512, 192])\n",
      "Parameter containing:\n",
      "tensor([[-0.0751, -0.0893,  0.0815,  ..., -0.0023,  0.0108,  0.0553],\n",
      "        [-0.0455,  0.0524, -0.0306,  ..., -0.0354, -0.0475, -0.0086],\n",
      "        [-0.0329,  0.0703, -0.0476,  ...,  0.0040,  0.0093, -0.0899],\n",
      "        ...,\n",
      "        [ 0.0060,  0.0643, -0.0282,  ...,  0.0184,  0.0319, -0.0253],\n",
      "        [-0.0182,  0.0615,  0.0271,  ...,  0.0675, -0.0561, -0.0581],\n",
      "        [-0.0358,  0.0303,  0.0649,  ..., -0.0089,  0.0761, -0.2177]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "[-0.07508604 -0.08928403  0.08154082 ... -0.00894041  0.07614809\n",
      " -0.21765612]\n",
      "Encoder0.bias Encoder0_b0\n",
      "(512,)\n",
      "torch.Size([512])\n",
      "Parameter containing:\n",
      "tensor([-0.1640, -0.3014, -0.1578, -0.0544, -0.2442, -0.4230, -0.0730, -0.1455,\n",
      "        -0.1958, -0.2338, -0.1662, -0.1184, -0.1595, -0.1054, -0.2449, -0.2744,\n",
      "         0.3854, -0.2818,  0.5248, -0.0677, -0.1090, -0.1178, -0.2102,  0.2377,\n",
      "         0.4789,  0.1886, -0.2035, -0.2202, -0.3630, -0.1779, -0.1787, -0.0984,\n",
      "        -0.1555, -0.1249, -0.1549, -0.3549, -0.1839, -0.1523, -0.1989, -0.1677,\n",
      "        -0.0936, -0.1551, -0.2757, -0.1913, -0.2227, -0.1848, -0.1450, -0.2108,\n",
      "         0.2748, -0.3838, -0.1906, -0.0878, -0.3309, -0.2950, -0.1489, -0.1433,\n",
      "        -0.0678, -0.0986, -0.3596, -0.3442, -0.1202, -0.1292, -0.2998, -0.3798,\n",
      "        -0.5403, -0.2838,  0.3112, -0.2935, -0.3343, -0.2571, -0.0496, -0.1544,\n",
      "        -0.1000, -0.3391, -0.3416, -0.2133, -0.2621, -0.3774, -0.1920, -0.1826,\n",
      "        -0.2558, -0.2018, -0.3717,  0.1867, -0.1788, -0.1508, -0.3846, -0.2013,\n",
      "        -0.1610, -0.1289,  0.4089, -0.1777, -0.3522, -0.1177,  0.5571, -0.3284,\n",
      "         0.5220, -0.1085, -0.1776, -0.1438, -0.2332, -0.1896, -0.2541, -0.2952,\n",
      "        -0.2068, -0.1214, -0.1102, -0.2018, -0.2112, -0.1603, -0.1218, -0.0671,\n",
      "        -0.2647,  0.1929, -0.2281, -0.1906, -0.1397, -0.1677, -0.3873, -0.1258,\n",
      "        -0.3395, -0.1583, -0.2389, -0.2625, -0.2412, -0.1295, -0.1360, -0.0635,\n",
      "        -0.1498, -0.1031, -0.1074, -0.2727, -0.1188, -0.1324, -0.2818, -0.0617,\n",
      "        -0.4059, -0.1478, -0.0956, -0.1221, -0.1219, -0.1803, -0.1824, -0.1016,\n",
      "         0.3265, -0.3195, -0.5142, -0.1727, -0.2644, -0.1785, -0.1639, -0.1529,\n",
      "        -0.1514, -0.2023,  0.1132, -0.1648, -0.2075, -0.1424, -0.1185, -0.3189,\n",
      "        -0.2751, -0.1782, -0.0904, -0.3374, -0.1831, -0.1108, -0.1497, -0.0749,\n",
      "        -0.2373,  0.5636, -0.1655, -0.1761, -0.1318, -0.1311, -0.1226, -0.3297,\n",
      "        -0.1105,  0.0322, -0.1865, -0.1200, -0.1420, -0.0131, -0.1520, -0.3672,\n",
      "        -0.2242, -0.1832, -0.1601, -0.4223, -0.1355, -0.1323,  0.1483, -0.0810,\n",
      "        -0.1676, -0.0816, -0.1722, -0.2645, -0.1598, -0.1274, -0.0785, -0.2377,\n",
      "        -0.1239,  0.3252, -0.1377, -0.0333, -0.0974, -0.2369, -0.4341, -0.1264,\n",
      "        -0.1963, -0.0879, -0.0705,  0.2595, -0.1618, -0.1682, -0.1282, -0.0621,\n",
      "        -0.1085, -0.1299, -0.1556,  0.6140, -0.3024, -0.1786,  0.2916, -0.1016,\n",
      "        -0.1383, -0.3019, -0.2828, -0.1330, -0.3842, -0.2050, -0.1671, -0.1569,\n",
      "        -0.1753, -0.2015, -0.0839, -0.3690, -0.5441, -0.3087, -0.2941, -0.2598,\n",
      "        -0.1406, -0.2503, -0.2124, -0.1061, -0.1275, -0.2483, -0.1663,  0.0573,\n",
      "        -0.1005, -0.2936, -0.2052, -0.2906, -0.1547,  0.1829, -0.1515, -0.2765,\n",
      "         0.2386, -0.1973,  0.3433, -0.2286, -0.2405, -0.1087, -0.1455, -0.1787,\n",
      "         0.2950, -0.3225, -0.0229, -0.1671, -0.1192,  0.3374,  0.2715, -0.1531,\n",
      "        -0.2548, -0.1279, -0.0361, -0.1904, -0.0837, -0.1230, -0.1748, -0.1108,\n",
      "        -0.1861, -0.2155, -0.3162, -0.2970, -0.2553, -0.2535, -0.2059,  0.3050,\n",
      "         0.2146, -0.2594, -0.1646, -0.1511,  0.0179, -0.2290, -0.1457, -0.0335,\n",
      "        -0.1483, -0.1141, -0.0747,  0.0856, -0.3069, -0.1689, -0.0301, -0.1490,\n",
      "        -0.2194, -0.2017, -0.1323, -0.4129, -0.2350, -0.2978, -0.1162, -0.3352,\n",
      "         0.3423, -0.1731, -0.1221, -0.1840, -0.2306, -0.1228, -0.1761, -0.2025,\n",
      "        -0.1910, -0.1375, -0.2802, -0.1821, -0.1476, -0.1194, -0.1308, -0.1568,\n",
      "        -0.2535, -0.2404, -0.3091, -0.1749, -0.1412, -0.1588, -0.1799, -0.1338,\n",
      "        -0.1635, -0.2909, -0.1468,  0.1187, -0.1731, -0.1908, -0.1999, -0.3052,\n",
      "        -0.0985, -0.3249, -0.1970, -0.3925, -0.2470, -0.2235, -0.1803, -0.1424,\n",
      "         0.3621, -0.1608, -0.2231, -0.1787, -0.2517, -0.1474, -0.3742,  0.1497,\n",
      "        -0.1302, -0.1326, -0.1354, -0.2421, -0.2031, -0.2032,  0.0543, -0.2035,\n",
      "        -0.2268, -0.3376, -0.2610, -0.1591, -0.1457, -0.2799, -0.2189, -0.1037,\n",
      "        -0.3752,  0.1436, -0.1019, -0.0661, -0.1344, -0.1216, -0.2023, -0.1113,\n",
      "        -0.1609, -0.3735, -0.1303,  0.1622, -0.2131, -0.2040, -0.1814, -0.1481,\n",
      "        -0.0888, -0.1660, -0.3547,  0.3531, -0.2156, -0.2752, -0.1976, -0.1453,\n",
      "        -0.2382, -0.2779, -0.0908, -0.2122, -0.2339, -0.0859, -0.3178, -0.2652,\n",
      "        -0.3201, -0.1810, -0.3678, -0.1718, -0.2538,  0.1322, -0.3014, -0.1906,\n",
      "        -0.1203, -0.1515, -0.1070, -0.2081, -0.2013, -0.1494, -0.1258, -0.3503,\n",
      "        -0.1271,  0.3416, -0.1348, -0.0689,  0.3573, -0.0028, -0.1705, -0.0953,\n",
      "        -0.1992, -0.1591,  0.0473, -0.1173, -0.2027, -0.1551, -0.1572,  0.2826,\n",
      "        -0.1706, -0.1679, -0.2174, -0.1593,  0.3117, -0.0169, -0.1073, -0.1380,\n",
      "        -0.1600, -0.0988, -0.1966, -0.3516, -0.1780, -0.1800, -0.2236, -0.1692,\n",
      "        -0.1073, -0.1010,  0.5641, -0.1769, -0.0206, -0.2577,  0.1222, -0.1628,\n",
      "        -0.2048, -0.3043, -0.1959, -0.1745, -0.3006, -0.2416,  0.3691, -0.1578,\n",
      "        -0.1074, -0.3076, -0.1280, -0.1990, -0.1435, -0.1874, -0.0948, -0.2300,\n",
      "        -0.1866, -0.2158, -0.0901, -0.1400,  0.0138,  0.3266, -0.0840, -0.3275,\n",
      "        -0.2286, -0.2583, -0.2955, -0.1522,  0.4923,  0.3641, -0.0486, -0.1660,\n",
      "         0.4059,  0.1804, -0.2585, -0.2407, -0.1350,  0.0404, -0.1234, -0.0744,\n",
      "        -0.1041, -0.1266,  0.2202,  0.2329, -0.0773, -0.3262, -0.1330, -0.2185],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "[-0.16396318 -0.30135894 -0.15779705 -0.05441843 -0.24419166 -0.42299247\n",
      " -0.07299345 -0.14546235 -0.19579217 -0.23376861 -0.16617714 -0.11837086\n",
      " -0.15949708 -0.10536314 -0.2448906  -0.2743575   0.38539436 -0.28177696\n",
      "  0.5247518  -0.06773113 -0.10899407 -0.11777221 -0.21023753  0.23765668\n",
      "  0.47885934  0.18859702 -0.20348333 -0.22016437 -0.3630036  -0.17792466\n",
      " -0.17866795 -0.09837939 -0.15554482 -0.12486947 -0.1549288  -0.35493147\n",
      " -0.18390448 -0.1523046  -0.19893137 -0.16765311 -0.09361651 -0.15505755\n",
      " -0.2757114  -0.19130859 -0.22265431 -0.18476535 -0.14497386 -0.2108336\n",
      "  0.27483085 -0.38380957 -0.19058977 -0.08783538 -0.33089736 -0.29502112\n",
      " -0.14892437 -0.14331438 -0.06777911 -0.09861241 -0.35962433 -0.34423485\n",
      " -0.12024388 -0.12917882 -0.2997554  -0.3798029  -0.5402979  -0.28378108\n",
      "  0.31118762 -0.29352155 -0.33429214 -0.2571203  -0.04964786 -0.1543892\n",
      " -0.09995038 -0.33908013 -0.34162632 -0.21333344 -0.26205617 -0.37736553\n",
      " -0.19204408 -0.18258071 -0.255768   -0.20175229 -0.3717136   0.18665549\n",
      " -0.17884499 -0.1507591  -0.38461795 -0.20130767 -0.16099557 -0.12887132\n",
      "  0.40893367 -0.17773284 -0.35223624 -0.11772762  0.55711997 -0.32843783\n",
      "  0.5219638  -0.10846656 -0.17755964 -0.14380276 -0.2331856  -0.18963887\n",
      " -0.2541282  -0.2952337  -0.20681392 -0.12142164 -0.11019765 -0.20181452\n",
      " -0.2111902  -0.16031517 -0.12182919 -0.06709661 -0.26474348  0.1928716\n",
      " -0.22812845 -0.19057423 -0.139738   -0.16767833 -0.3872909  -0.125771\n",
      " -0.33945814 -0.15828955 -0.23886587 -0.26254615 -0.24115397 -0.1295045\n",
      " -0.13604549 -0.06351275 -0.14977868 -0.1030627  -0.10740916 -0.27272934\n",
      " -0.11879658 -0.13237815 -0.28181243 -0.06171714 -0.4059031  -0.14776948\n",
      " -0.09555714 -0.12205976 -0.12186775 -0.18031783 -0.1823793  -0.10159438\n",
      "  0.32651848 -0.319507   -0.5142203  -0.1727008  -0.26439342 -0.17851858\n",
      " -0.1639457  -0.15293369 -0.1514175  -0.20227467  0.1132351  -0.16479212\n",
      " -0.2074597  -0.1423752  -0.11852391 -0.31892216 -0.27509984 -0.1781594\n",
      " -0.09037308 -0.33738616 -0.183118   -0.11079498 -0.14969754 -0.07494604\n",
      " -0.23732083  0.5636362  -0.16546318 -0.1760752  -0.13177714 -0.13112739\n",
      " -0.12255745 -0.32974023 -0.11045986  0.03216098 -0.18647835 -0.12001375\n",
      " -0.14204608 -0.01306684 -0.15202448 -0.36723548 -0.22424106 -0.18318368\n",
      " -0.16010877 -0.42230904 -0.1355005  -0.13228467  0.14832836 -0.08102485\n",
      " -0.16762444 -0.0815641  -0.17222239 -0.26445973 -0.15984482 -0.12737593\n",
      " -0.07846613 -0.23768243 -0.12390457  0.3252234  -0.13773097 -0.03331687\n",
      " -0.09738965 -0.2368899  -0.4341269  -0.12636042 -0.19628242 -0.08789015\n",
      " -0.07052194  0.2595158  -0.16180378 -0.1682127  -0.12824543 -0.06205293\n",
      " -0.10852116 -0.12992077 -0.15557905  0.6140445  -0.30243468 -0.17862986\n",
      "  0.29159933 -0.10158155 -0.13833585 -0.30186912 -0.28277457 -0.13296199\n",
      " -0.3842154  -0.20498598 -0.16707109 -0.15685779 -0.1753407  -0.20150958\n",
      " -0.08389562 -0.36900967 -0.5440759  -0.30865765 -0.29411885 -0.25975412\n",
      " -0.14062446 -0.2503367  -0.21239823 -0.10605614 -0.12749548 -0.24834706\n",
      " -0.16626893  0.0573024  -0.10047532 -0.2936257  -0.20516473 -0.2905557\n",
      " -0.15468214  0.1829484  -0.15149415 -0.27646926  0.23862909 -0.19727421\n",
      "  0.34326676 -0.22855465 -0.24048322 -0.10867462 -0.1455349  -0.17871727\n",
      "  0.29500216 -0.32254297 -0.02293776 -0.1670832  -0.11920563  0.3373827\n",
      "  0.2714754  -0.15310459 -0.2548017  -0.12792476 -0.03614244 -0.19035077\n",
      " -0.08368215 -0.12301761 -0.17480399 -0.11081686 -0.18612339 -0.2155204\n",
      " -0.31624338 -0.2969748  -0.25532806 -0.2535061  -0.2059107   0.3049631\n",
      "  0.2145656  -0.25935596 -0.16463926 -0.15113138  0.01786781 -0.22901045\n",
      " -0.14574437 -0.03346995 -0.14825684 -0.11408152 -0.07474849  0.08555812\n",
      " -0.30688426 -0.16887306 -0.03007207 -0.14898346 -0.2194225  -0.20172475\n",
      " -0.13233817 -0.412936   -0.23503938 -0.29776558 -0.11621572 -0.33524707\n",
      "  0.34232816 -0.17313449 -0.12211022 -0.18396592 -0.23058134 -0.1227833\n",
      " -0.17610991 -0.20253897 -0.19099507 -0.13750127 -0.28018188 -0.18209311\n",
      " -0.14764811 -0.11939019 -0.13079317 -0.15676051 -0.25354168 -0.24035569\n",
      " -0.3090708  -0.17487355 -0.14119846 -0.1587804  -0.17989066 -0.13380644\n",
      " -0.16348362 -0.29087025 -0.146765    0.11871413 -0.17310357 -0.19080259\n",
      " -0.19985923 -0.3051612  -0.09852522 -0.3249023  -0.196971   -0.3925146\n",
      " -0.24699272 -0.2234606  -0.18026114 -0.14242443  0.36205035 -0.16082205\n",
      " -0.22310723 -0.17871754 -0.25173366 -0.1474093  -0.37423995  0.14972226\n",
      " -0.13020077 -0.13256623 -0.13535531 -0.24205194 -0.2031234  -0.20320266\n",
      "  0.05433664 -0.2034639  -0.22682716 -0.33764693 -0.26103818 -0.15914446\n",
      " -0.14574763 -0.2799041  -0.21890129 -0.10374285 -0.3752293   0.14364699\n",
      " -0.10186159 -0.06614887 -0.1344497  -0.12158894 -0.20226793 -0.11133078\n",
      " -0.160938   -0.37353465 -0.13028346  0.16223073 -0.21314158 -0.20401064\n",
      " -0.18142498 -0.1481066  -0.08878003 -0.1660257  -0.3546506   0.35307482\n",
      " -0.2156157  -0.27523467 -0.19763279 -0.14526619 -0.23820113 -0.27790877\n",
      " -0.09078615 -0.21215358 -0.2339236  -0.08593812 -0.31777513 -0.2651932\n",
      " -0.32009384 -0.18095505 -0.36782616 -0.17177826 -0.25375807  0.1322374\n",
      " -0.30135885 -0.19058429 -0.12034962 -0.1515302  -0.10700005 -0.20809235\n",
      " -0.2012574  -0.14941353 -0.12584849 -0.3503398  -0.12711447  0.3415683\n",
      " -0.13477209 -0.0689173   0.35732388 -0.00282006 -0.17045629 -0.09526154\n",
      " -0.1991896  -0.15909459  0.04729273 -0.11733647 -0.20274112 -0.15511213\n",
      " -0.15721224  0.28264514 -0.17060544 -0.1679276  -0.21738723 -0.1592729\n",
      "  0.3116945  -0.01693004 -0.10727283 -0.13804857 -0.16002609 -0.09881424\n",
      " -0.1966377  -0.3516431  -0.17803699 -0.18000276 -0.22362839 -0.16921416\n",
      " -0.10727398 -0.10096347  0.5640756  -0.17688517 -0.02059263 -0.2576959\n",
      "  0.12220559 -0.16283159 -0.2048461  -0.30430028 -0.19585778 -0.17447153\n",
      " -0.30060822 -0.24164017  0.36907405 -0.15783884 -0.10739586 -0.3076439\n",
      " -0.12802467 -0.19904198 -0.14352776 -0.18739827 -0.09479127 -0.22996774\n",
      " -0.18655175 -0.21580988 -0.09006716 -0.1400052   0.01376048  0.3266382\n",
      " -0.08397542 -0.32745135 -0.22864714 -0.25825208 -0.29547352 -0.15216058\n",
      "  0.49232644  0.36411494 -0.04861686 -0.16599417  0.40591556  0.1803839\n",
      " -0.25854108 -0.24073371 -0.13500462  0.0403961  -0.12335188 -0.0743586\n",
      " -0.10413722 -0.12656264  0.220163    0.2329281  -0.07731631 -0.32620916\n",
      " -0.13295951 -0.2184715 ]\n",
      "Encoder1.weight Encoder1_w0\n",
      "(172032,)\n",
      "torch.Size([512, 336])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0151, -0.0454, -0.0045,  ...,  0.0743, -0.0504, -0.0051],\n",
      "        [-0.0205, -0.0371,  0.0407,  ...,  0.0083, -0.0203, -0.0291],\n",
      "        [-0.0443,  0.0076, -0.0095,  ...,  0.1006, -0.0940,  0.0676],\n",
      "        ...,\n",
      "        [-0.0461, -0.0345,  0.0136,  ..., -0.0455, -0.0179, -0.0300],\n",
      "        [ 0.0266,  0.0437,  0.1292,  ...,  0.0437, -0.0176, -0.0445],\n",
      "        [ 0.0074, -0.0037, -0.0335,  ..., -0.0343, -0.0006,  0.0782]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "[ 0.01510694 -0.0454176  -0.00447502 ... -0.03431524 -0.000599\n",
      "  0.07818131]\n",
      "Encoder1.bias Encoder1_b0\n",
      "(512,)\n",
      "torch.Size([512])\n",
      "Parameter containing:\n",
      "tensor([-0.2406, -0.3507, -0.3916, -0.3398, -0.1664, -0.2168, -0.3187, -0.3043,\n",
      "        -0.2258, -0.2667, -0.2999, -0.2634, -0.2336, -0.1910, -0.1900, -0.2181,\n",
      "        -0.2915, -0.3086, -0.3763, -0.3578, -0.3352, -0.3185, -0.3127, -0.3608,\n",
      "        -0.3058, -0.3229, -0.3290, -0.2595, -0.3054, -0.2928, -0.3379, -0.2918,\n",
      "        -0.3088, -0.3166, -0.2562, -0.2982, -0.2441, -0.2535, -0.2273, -0.2835,\n",
      "        -0.2893, -0.2202, -0.2721, -0.2743, -0.3047, -0.3187, -0.3952, -0.3036,\n",
      "        -0.3886, -0.2557, -0.2231, -0.2757, -0.2194, -0.2295, -0.2829, -0.2976,\n",
      "        -0.2673, -0.2423, -0.3264, -0.2856, -0.3466, -0.2459, -0.2643, -0.2893,\n",
      "        -0.2956, -0.2841, -0.3279, -0.2971, -0.3203, -0.2207, -0.1854, -0.2690,\n",
      "        -0.2631, -0.3303, -0.3290, -0.2560, -0.3221, -0.2649, -0.2291, -0.3176,\n",
      "        -0.3128, -0.2931, -0.3504, -0.2946, -0.2942, -0.2593, -0.2591, -0.4058,\n",
      "        -0.2106, -0.2383, -0.2301, -0.3027, -0.2607, -0.2684, -0.2358, -0.3877,\n",
      "        -0.2492, -0.2645, -0.2877, -0.1611, -0.2774, -0.2394, -0.2964, -0.3116,\n",
      "        -0.3448, -0.3188, -0.2066, -0.2847, -0.2653, -0.2685, -0.2913, -0.2910,\n",
      "        -0.2426, -0.4096, -0.2006, -0.1753, -0.2774, -0.2912, -0.1771, -0.2589,\n",
      "        -0.2575, -0.3169, -0.2161, -0.3139, -0.3249, -0.1956, -0.2385, -0.3595,\n",
      "        -0.2717, -0.3333, -0.1857, -0.2346, -0.3649, -0.2224, -0.1987, -0.2826,\n",
      "        -0.2383, -0.3042, -0.3339, -0.2686, -0.2948, -0.1815, -0.3564, -0.2522,\n",
      "        -0.2459, -0.2977, -0.2945, -0.3080, -0.1768, -0.2161, -0.3401, -0.2408,\n",
      "        -0.3114, -0.2017, -0.2501, -0.2523, -0.3648, -0.3045, -0.3518, -0.2396,\n",
      "        -0.2699, -0.2609, -0.2115, -0.2208, -0.2989, -0.3622, -0.3623, -0.3209,\n",
      "        -0.2514, -0.2675, -0.3922, -0.2846, -0.3482, -0.2488, -0.2901, -0.3162,\n",
      "        -0.3048, -0.2571, -0.2829, -0.3231, -0.2369, -0.2796, -0.2078, -0.2013,\n",
      "        -0.3201, -0.2192, -0.2517, -0.2549, -0.2820, -0.2952, -0.2775, -0.3310,\n",
      "        -0.2835, -0.2793, -0.2575, -0.2253, -0.1165, -0.2508, -0.2040, -0.2104,\n",
      "        -0.2874, -0.3231, -0.2870, -0.2359, -0.1772, -0.2084, -0.1688, -0.3293,\n",
      "        -0.2962, -0.2373, -0.3500, -0.2376, -0.2386, -0.2387, -0.3689, -0.3092,\n",
      "        -0.3319, -0.2399, -0.3394, -0.2648, -0.2687, -0.3430, -0.3139, -0.2759,\n",
      "        -0.3031, -0.2419, -0.2509, -0.2833, -0.2058, -0.2706, -0.3415, -0.2962,\n",
      "        -0.2648, -0.3013, -0.2963, -0.3090, -0.2060, -0.3443, -0.3161, -0.2730,\n",
      "        -0.3868, -0.2896, -0.3272, -0.2118, -0.2835, -0.2692, -0.2939, -0.2182,\n",
      "        -0.2797, -0.2721, -0.1882, -0.3592, -0.2579, -0.2701, -0.3540, -0.4596,\n",
      "        -0.2629, -0.2788, -0.3492, -0.3603, -0.2716, -0.3488, -0.2716, -0.3258,\n",
      "        -0.3444, -0.2791, -0.2627, -0.2705, -0.2042, -0.2580, -0.1648, -0.2392,\n",
      "        -0.3101, -0.2479, -0.2717, -0.2574, -0.3061, -0.3664, -0.3465, -0.4245,\n",
      "        -0.2217, -0.3251, -0.2873, -0.3515, -0.3135, -0.2647, -0.4180, -0.2580,\n",
      "        -0.3778, -0.2709, -0.2371, -0.2693, -0.2734, -0.3207, -0.2425, -0.3828,\n",
      "        -0.3173, -0.2856, -0.2337, -0.3078, -0.1829, -0.2779, -0.3089, -0.3543,\n",
      "        -0.3167, -0.2950, -0.1015, -0.4058, -0.2448, -0.1528, -0.2801, -0.3152,\n",
      "        -0.2783, -0.2102, -0.3113, -0.2190, -0.2637, -0.1088, -0.2743, -0.1439,\n",
      "        -0.2998, -0.2703, -0.2665, -0.3873, -0.2004, -0.2914, -0.3378, -0.3053,\n",
      "        -0.3154, -0.2528, -0.3530, -0.2748, -0.2513, -0.3288, -0.2224, -0.2891,\n",
      "        -0.2573, -0.3120, -0.1725, -0.2479, -0.3111, -0.1921, -0.3139, -0.2054,\n",
      "        -0.2225, -0.4233, -0.3038, -0.3024, -0.3082, -0.2751, -0.2238, -0.2635,\n",
      "        -0.2286, -0.3047, -0.3154, -0.2448, -0.3704, -0.2979, -0.3988, -0.2494,\n",
      "        -0.3142, -0.2824, -0.2838, -0.3681, -0.1838, -0.2547, -0.3132, -0.2270,\n",
      "        -0.3749, -0.3189, -0.3063, -0.3214, -0.3353, -0.2597, -0.2356, -0.2825,\n",
      "        -0.3089, -0.1848, -0.3726, -0.2424, -0.2458, -0.2746, -0.3124, -0.3234,\n",
      "        -0.2245, -0.2481, -0.2030, -0.2533, -0.2738, -0.3700, -0.2214, -0.3937,\n",
      "        -0.2748, -0.2174, -0.2455, -0.2605, -0.2641, -0.2816, -0.2792, -0.2412,\n",
      "        -0.3654, -0.2991, -0.2753, -0.2679, -0.3134, -0.2765, -0.3176, -0.2621,\n",
      "        -0.4285, -0.2706, -0.2219, -0.2882, -0.2676, -0.4346, -0.2007, -0.2786,\n",
      "        -0.3178, -0.2547, -0.2809, -0.3347, -0.2870, -0.2956, -0.2264, -0.4821,\n",
      "        -0.2760, -0.2902, -0.3459, -0.2625, -0.2989, -0.2847, -0.2756, -0.2453,\n",
      "        -0.1895, -0.3156, -0.2073, -0.2254, -0.3555, -0.2597, -0.2759, -0.2888,\n",
      "        -0.2701, -0.3078, -0.2921, -0.3380, -0.3340, -0.2927, -0.2363, -0.2386,\n",
      "        -0.2770, -0.3205, -0.3443, -0.3745, -0.2709, -0.2635, -0.2527, -0.2841,\n",
      "        -0.2561, -0.3294, -0.2752, -0.1849, -0.2488, -0.2311, -0.3712, -0.2793,\n",
      "        -0.3499, -0.2513, -0.2582, -0.2544, -0.1964, -0.2909, -0.2058, -0.2302,\n",
      "        -0.2437, -0.2734, -0.2576, -0.2507, -0.2791, -0.2990, -0.2478, -0.2493,\n",
      "        -0.2205, -0.3412, -0.1456, -0.2653, -0.3025, -0.2237, -0.2385, -0.2448,\n",
      "        -0.2238, -0.3748, -0.2817, -0.2220, -0.2755, -0.3670, -0.2753, -0.2309,\n",
      "        -0.2302, -0.2456, -0.2341, -0.2862, -0.2272, -0.2411, -0.3351, -0.2707,\n",
      "        -0.2685, -0.2414, -0.3210, -0.2534, -0.2477, -0.2015, -0.3378, -0.2217],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "[-0.24058904 -0.35072136 -0.39164218 -0.3397575  -0.16642681 -0.21678117\n",
      " -0.31870937 -0.30425316 -0.22579522 -0.26667115 -0.29988745 -0.26337773\n",
      " -0.23361157 -0.19103622 -0.19004312 -0.21809334 -0.29146028 -0.30862987\n",
      " -0.3763192  -0.35781705 -0.3352049  -0.3184942  -0.3126803  -0.3607571\n",
      " -0.30579588 -0.32288668 -0.3290101  -0.2595174  -0.30539572 -0.29283738\n",
      " -0.33789057 -0.29181844 -0.30876178 -0.31659907 -0.25620434 -0.29822412\n",
      " -0.24409835 -0.25345257 -0.22727042 -0.28351146 -0.28933594 -0.22015305\n",
      " -0.272109   -0.27432695 -0.3047113  -0.31865847 -0.39521262 -0.3036259\n",
      " -0.38861537 -0.25570092 -0.22310324 -0.27567875 -0.21944009 -0.22946621\n",
      " -0.2828847  -0.29762876 -0.26733363 -0.24234186 -0.326437   -0.28556222\n",
      " -0.3466472  -0.24585196 -0.26433852 -0.28925103 -0.29558873 -0.28407174\n",
      " -0.32792684 -0.29709923 -0.32028258 -0.2206837  -0.18544193 -0.2690404\n",
      " -0.26310354 -0.3303025  -0.32903033 -0.25603357 -0.32211792 -0.26488054\n",
      " -0.22910586 -0.3175591  -0.31284505 -0.29312047 -0.3504039  -0.29464033\n",
      " -0.2941595  -0.25926962 -0.25906196 -0.40578708 -0.21064131 -0.23830354\n",
      " -0.23012632 -0.3026741  -0.26065078 -0.26835686 -0.23576805 -0.38773876\n",
      " -0.2491765  -0.2645355  -0.2877282  -0.16112727 -0.27740508 -0.23938756\n",
      " -0.29636797 -0.31156924 -0.3448081  -0.31884065 -0.20664236 -0.28469643\n",
      " -0.26530898 -0.2684978  -0.29127628 -0.29103455 -0.24259768 -0.40956557\n",
      " -0.20062768 -0.17527618 -0.27739576 -0.29121977 -0.17712997 -0.25885823\n",
      " -0.25748375 -0.31691697 -0.2161232  -0.31391937 -0.3249431  -0.195568\n",
      " -0.2385318  -0.3594554  -0.27169707 -0.33332506 -0.18570015 -0.23460081\n",
      " -0.36494878 -0.22242165 -0.19870505 -0.2825828  -0.2382659  -0.3041528\n",
      " -0.33390927 -0.26857042 -0.29482174 -0.1815466  -0.3563684  -0.25219277\n",
      " -0.24585551 -0.29773766 -0.29451504 -0.308032   -0.1768302  -0.21608454\n",
      " -0.3400775  -0.24083997 -0.31141406 -0.20172225 -0.25014478 -0.25227696\n",
      " -0.3648242  -0.30454963 -0.35178208 -0.23959202 -0.2699385  -0.26088312\n",
      " -0.2115306  -0.22077374 -0.29885098 -0.36216202 -0.36228752 -0.3209402\n",
      " -0.2514248  -0.2674814  -0.3921563  -0.28457454 -0.34816954 -0.24882345\n",
      " -0.29005158 -0.3162425  -0.30483037 -0.25712523 -0.28294015 -0.3231327\n",
      " -0.23685257 -0.27959964 -0.2078117  -0.20133033 -0.32009313 -0.21919584\n",
      " -0.25171876 -0.25494567 -0.28204042 -0.29523593 -0.27751172 -0.3310466\n",
      " -0.28345358 -0.27934217 -0.2574605  -0.22525297 -0.11651617 -0.25079522\n",
      " -0.20404    -0.2103966  -0.2874371  -0.32314703 -0.28701422 -0.23594092\n",
      " -0.17723946 -0.20835486 -0.16881819 -0.32932776 -0.29615363 -0.2372956\n",
      " -0.3500029  -0.23759188 -0.23862645 -0.2386774  -0.36894357 -0.30919555\n",
      " -0.33185473 -0.23991977 -0.33935115 -0.26476404 -0.26874995 -0.34303942\n",
      " -0.31388718 -0.27594033 -0.30313286 -0.24191304 -0.25094447 -0.2832799\n",
      " -0.20584157 -0.2705931  -0.3415448  -0.29620415 -0.26482975 -0.30134812\n",
      " -0.29627448 -0.30903864 -0.20600656 -0.34433267 -0.3160996  -0.27300665\n",
      " -0.38683584 -0.28963175 -0.32723328 -0.21175317 -0.28347617 -0.26924053\n",
      " -0.29386538 -0.21819341 -0.2797004  -0.27208373 -0.18820086 -0.3591663\n",
      " -0.25794134 -0.27006257 -0.35399225 -0.45956036 -0.26286563 -0.27881384\n",
      " -0.34918284 -0.36027142 -0.2715813  -0.34876332 -0.27163914 -0.3257581\n",
      " -0.34444025 -0.27913928 -0.26271626 -0.27047777 -0.2042402  -0.2579869\n",
      " -0.16475831 -0.2391599  -0.3100611  -0.24787846 -0.27174577 -0.257356\n",
      " -0.30610082 -0.36639223 -0.3464513  -0.42454094 -0.2216571  -0.32510993\n",
      " -0.28726605 -0.35154092 -0.31352016 -0.2646861  -0.4180119  -0.25795564\n",
      " -0.3778462  -0.27092454 -0.23708785 -0.26933795 -0.27344614 -0.3206986\n",
      " -0.24245453 -0.38279325 -0.3172599  -0.2856169  -0.23366827 -0.30783108\n",
      " -0.1829149  -0.2779348  -0.30893457 -0.35434204 -0.31673944 -0.2949592\n",
      " -0.10148694 -0.4057901  -0.24476025 -0.15278842 -0.28013068 -0.31520766\n",
      " -0.27833784 -0.2101683  -0.3112937  -0.21897453 -0.2636796  -0.10881644\n",
      " -0.27432746 -0.14390889 -0.29984972 -0.2703167  -0.26648843 -0.38731557\n",
      " -0.20038855 -0.29144487 -0.33780065 -0.30526462 -0.3153814  -0.25277457\n",
      " -0.3530062  -0.2747572  -0.25132012 -0.32876623 -0.22238699 -0.28910127\n",
      " -0.25733194 -0.3120068  -0.17254068 -0.24792336 -0.31113884 -0.19212334\n",
      " -0.31389517 -0.20535187 -0.22252628 -0.42334023 -0.30378628 -0.30241606\n",
      " -0.3081708  -0.27509934 -0.22384132 -0.26345712 -0.22857513 -0.30471438\n",
      " -0.31540176 -0.24483682 -0.3704107  -0.2978625  -0.3988084  -0.24940203\n",
      " -0.31420946 -0.28243443 -0.2838264  -0.36808524 -0.18382227 -0.25469062\n",
      " -0.31316653 -0.22700508 -0.37493804 -0.31891546 -0.30630302 -0.32137075\n",
      " -0.33527884 -0.25967985 -0.23562329 -0.28252327 -0.30894205 -0.18480636\n",
      " -0.37264356 -0.24237874 -0.24581611 -0.274616   -0.312447   -0.32343444\n",
      " -0.22451241 -0.24811356 -0.20304404 -0.2533182  -0.2738257  -0.37004435\n",
      " -0.22136128 -0.39369    -0.27476174 -0.2173809  -0.24546531 -0.26045737\n",
      " -0.26413417 -0.28163314 -0.27918994 -0.24123833 -0.36536905 -0.29908633\n",
      " -0.27531222 -0.26786664 -0.31342056 -0.27647734 -0.31760624 -0.2621398\n",
      " -0.42852947 -0.27061155 -0.22188884 -0.28822762 -0.26760763 -0.4345903\n",
      " -0.20065118 -0.2785513  -0.31775555 -0.2547107  -0.2808629  -0.334673\n",
      " -0.28695247 -0.29558676 -0.22636555 -0.4820712  -0.27597612 -0.29023647\n",
      " -0.3459085  -0.26250014 -0.29888424 -0.28468424 -0.2755752  -0.24531627\n",
      " -0.18953712 -0.31563523 -0.20728339 -0.2253772  -0.35548094 -0.2596657\n",
      " -0.27594945 -0.2887708  -0.2701497  -0.3077938  -0.29210737 -0.33799088\n",
      " -0.33398858 -0.29271865 -0.23627064 -0.23855329 -0.27696106 -0.32054466\n",
      " -0.344254   -0.37451413 -0.2709444  -0.26348537 -0.25265217 -0.28414106\n",
      " -0.25612882 -0.32942137 -0.27524358 -0.18487132 -0.24875526 -0.23106004\n",
      " -0.3712261  -0.27926543 -0.34993923 -0.251304   -0.25815448 -0.25443912\n",
      " -0.19639981 -0.29090264 -0.20582993 -0.23018144 -0.24371451 -0.27339312\n",
      " -0.25757018 -0.25066674 -0.2790643  -0.2989604  -0.24776098 -0.24927801\n",
      " -0.22047248 -0.34120595 -0.14556223 -0.26525536 -0.3024658  -0.22374961\n",
      " -0.23845993 -0.2447581  -0.22375962 -0.37480175 -0.28168055 -0.22200164\n",
      " -0.27549967 -0.36695957 -0.2752641  -0.23087215 -0.23018874 -0.24558945\n",
      " -0.23407476 -0.2862332  -0.22717953 -0.24107362 -0.33505002 -0.2707437\n",
      " -0.26846054 -0.2413681  -0.32104442 -0.25344452 -0.24768372 -0.20154089\n",
      " -0.33784992 -0.22171423]\n",
      "Encoder2.weight Encoder2_w0\n",
      "(932864,)\n",
      "torch.Size([512, 1822])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0200,  0.0423,  0.0208,  ..., -0.1259, -0.0879,  0.1175],\n",
      "        [ 0.0125,  0.0227, -0.0237,  ..., -0.0761, -0.0549,  0.0368],\n",
      "        [-0.0261,  0.0089,  0.0344,  ..., -0.0815, -0.0520,  0.0535],\n",
      "        ...,\n",
      "        [-0.0227, -0.0446,  0.0473,  ...,  0.0373,  0.0220,  0.0109],\n",
      "        [-0.0153,  0.0379, -0.0243,  ..., -0.0871,  0.0020,  0.0594],\n",
      "        [-0.0226,  0.0160,  0.0045,  ...,  0.0135, -0.0835, -0.0419]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "[ 0.01998104  0.04234988  0.02081621 ...  0.01352898 -0.08354721\n",
      " -0.04188442]\n",
      "Encoder2.bias Encoder2_b0\n",
      "(512,)\n",
      "torch.Size([512])\n",
      "Parameter containing:\n",
      "tensor([-0.4657, -0.4660, -0.3886, -0.4677, -0.5336, -0.4965, -0.2057, -0.5277,\n",
      "        -0.3778, -0.2528, -0.4298, -0.4074, -0.4896, -0.1427, -0.4637, -0.2209,\n",
      "        -0.5543, -0.4108, -0.4896, -0.4101, -0.4016, -0.3938, -0.3264, -0.4346,\n",
      "        -0.3746, -0.3749, -0.3408, -0.5137, -0.2973, -0.4652, -0.3371, -0.5543,\n",
      "        -0.3108, -0.4166, -0.4316, -0.4849, -0.3625, -0.3658, -0.1957, -0.3417,\n",
      "        -0.4405, -0.2911, -0.3479, -0.5099, -0.3606, -0.4265, -0.4124, -0.4777,\n",
      "        -0.1638, -0.2419, -0.3505, -0.3537, -0.2696, -0.5733, -0.4262, -0.6567,\n",
      "        -0.5222, -0.3293, -0.3194, -0.3493, -0.4433, -0.4817, -0.4970, -0.5509,\n",
      "        -0.5072, -0.2450, -0.3920, -0.4915, -0.3109, -0.4045, -0.3764, -0.3480,\n",
      "        -0.3586, -0.4798, -0.3316, -0.5762, -0.3390, -0.2324, -0.4506, -0.4077,\n",
      "        -0.4168, -0.3523, -0.3871, -0.3315, -0.4120, -0.5693, -0.5574, -0.4822,\n",
      "        -0.3533, -0.3825, -0.5047, -0.4870, -0.3616, -0.3773, -0.3700, -0.4474,\n",
      "        -0.4518, -0.4075, -0.4436, -0.5470, -0.3511, -0.4956, -0.4814, -0.4172,\n",
      "        -0.3598, -0.4927, -0.5024, -0.2544, -0.4308, -0.4310, -0.4548, -0.5528,\n",
      "        -0.4441, -0.3979, -0.5427, -0.2635, -0.3585, -0.4404, -0.5596, -0.2928,\n",
      "        -0.4967, -0.4453, -0.3862, -0.3865, -0.4491, -0.3378, -0.4297, -0.4743,\n",
      "        -0.3790, -0.3833, -0.4986, -0.4916, -0.4519, -0.3621, -0.4993, -0.2924,\n",
      "        -0.5141, -0.4249, -0.5345, -0.2566, -0.4632, -0.4946, -0.4985, -0.4977,\n",
      "        -0.3868, -0.4437, -0.2354, -0.4090, -0.2512, -0.3803, -0.3284, -0.2832,\n",
      "        -0.3848, -0.2179, -0.3352, -0.3702, -0.3975, -0.4748, -0.2854, -0.3557,\n",
      "        -0.3877, -0.4736, -0.2988, -0.4973, -0.3997, -0.4266, -0.4324, -0.4218,\n",
      "        -0.4996, -0.4682, -0.1848, -0.4375, -0.3895, -0.3715, -0.4734, -0.3627,\n",
      "        -0.3532, -0.4111, -0.4042, -0.3270, -0.3450, -0.3548, -0.4676, -0.4343,\n",
      "        -0.4063, -0.5602, -0.4533, -0.4924, -0.3657, -0.3993, -0.2625, -0.1929,\n",
      "        -0.2943, -0.4846, -0.3525, -0.3139, -0.3162, -0.4013, -0.3858, -0.3562,\n",
      "        -0.4845, -0.4005, -0.4134, -0.3716, -0.4068, -0.3544, -0.2895, -0.4940,\n",
      "        -0.4815, -0.5668, -0.4824, -0.5052, -0.3597, -0.4894, -0.3560, -0.4047,\n",
      "        -0.3621, -0.4124, -0.4140, -0.4161, -0.5279, -0.4773, -0.4355, -0.3816,\n",
      "        -0.4478, -0.3141, -0.4108, -0.2873, -0.4008, -0.4736, -0.4760, -0.3985,\n",
      "        -0.4614, -0.3891, -0.2596, -0.3968, -0.3715, -0.2214, -0.3257, -0.2781,\n",
      "        -0.5303, -0.3580, -0.6299, -0.4192, -0.4482, -0.5192, -0.4000, -0.4900,\n",
      "        -0.4640, -0.4749, -0.4909, -0.4419, -0.4684, -0.3089, -0.3865, -0.1080,\n",
      "        -0.3116, -0.5191, -0.3792, -0.4160, -0.4448, -0.4369, -0.3834, -0.3138,\n",
      "        -0.4685, -0.4179, -0.3246, -0.4236, -0.4647, -0.5087, -0.4053, -0.4386,\n",
      "        -0.2420, -0.1494, -0.5081, -0.3783, -0.3025, -0.4828, -0.5885, -0.3552,\n",
      "        -0.2837, -0.3152, -0.0609, -0.3070, -0.4041, -0.3950, -0.3559, -0.3122,\n",
      "        -0.5563, -0.3823, -0.3002, -0.5347, -0.4113, -0.3264, -0.4469, -0.4855,\n",
      "        -0.4096, -0.3759, -0.4717, -0.3985, -0.4380, -0.4758, -0.5476, -0.3287,\n",
      "        -0.4148, -0.3910, -0.3397, -0.5430, -0.4006, -0.4457, -0.4350, -0.4257,\n",
      "        -0.5790, -0.4533, -0.3174, -0.2328, -0.4043, -0.3718, -0.2789, -0.3619,\n",
      "        -0.3686, -0.2845, -0.3619, -0.4240, -0.4420, -0.4834, -0.3334, -0.3576,\n",
      "        -0.4297, -0.4723, -0.4604, -0.5043, -0.4728, -0.2623, -0.3762, -0.4755,\n",
      "        -0.4003, -0.3670, -0.3885, -0.3228, -0.4661, -0.4989, -0.3280, -0.4513,\n",
      "        -0.3835, -0.5264, -0.3169, -0.2597, -0.4450, -0.4662, -0.4570, -0.0664,\n",
      "        -0.5292, -0.4871, -0.4208, -0.4315, -0.4818, -0.5224, -0.4239, -0.4064,\n",
      "        -0.4483, -0.3269, -0.3353, -0.4020, -0.4661, -0.3786, -0.5754, -0.4153,\n",
      "        -0.3560, -0.4726, -0.5006, -0.3396, -0.2687, -0.5098, -0.4326, -0.5317,\n",
      "        -0.4505, -0.4548, -0.5121, -0.4795, -0.4499, -0.3033, -0.4627, -0.2938,\n",
      "        -0.3230, -0.3208, -0.4302, -0.4238, -0.3244, -0.4346, -0.4997, -0.5303,\n",
      "        -0.3980, -0.4517, -0.3584, -0.4102, -0.3306, -0.3805, -0.3248, -0.4242,\n",
      "        -0.4505, -0.2962, -0.4424, -0.4511, -0.3482, -0.5288, -0.5051, -0.3899,\n",
      "        -0.2979, -0.4952, -0.4372, -0.2274, -0.4347, -0.4565, -0.5522, -0.5103,\n",
      "        -0.3234, -0.5403, -0.3865, -0.4386, -0.3898, -0.5241, -0.4937, -0.0740,\n",
      "        -0.4733, -0.3011, -0.4514, -0.4768, -0.3235, -0.5214, -0.3109, -0.3749,\n",
      "        -0.5300, -0.4160, -0.4940, -0.3693, -0.4351, -0.3030, -0.5215, -0.5125,\n",
      "        -0.1700, -0.5078, -0.4062, -0.4263, -0.3888, -0.3985, -0.4720, -0.4996,\n",
      "        -0.4135, -0.4584, -0.4872, -0.3822, -0.4052, -0.4461, -0.4577, -0.3276,\n",
      "        -0.4472, -0.4052, -0.4403, -0.3813, -0.4314, -0.3042, -0.4523, -0.4499,\n",
      "        -0.2836, -0.5099, -0.4189, -0.3709, -0.3735, -0.3364, -0.2808, -0.3795,\n",
      "        -0.4300, -0.5054, -0.4033, -0.3660, -0.4179, -0.4808, -0.4924, -0.4967,\n",
      "        -0.3779, -0.4269, -0.3742, -0.4034, -0.4590, -0.3827, -0.3790, -0.4835,\n",
      "        -0.4901, -0.4683, -0.3562, -0.6567, -0.5062, -0.5385, -0.5189, -0.4484,\n",
      "        -0.3567, -0.2954, -0.2724, -0.4423, -0.3375, -0.4119, -0.4776, -0.4401,\n",
      "        -0.4234, -0.5510, -0.4813, -0.4206, -0.2522, -0.5231, -0.4513, -0.5105],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "[-0.4656942  -0.46600026 -0.38862076 -0.46769914 -0.5336008  -0.4965143\n",
      " -0.20569995 -0.527717   -0.37778497 -0.25282314 -0.4298219  -0.4074289\n",
      " -0.48957056 -0.14271814 -0.46370625 -0.22092463 -0.55434614 -0.41079098\n",
      " -0.48962682 -0.41012692 -0.40155512 -0.39377996 -0.32641196 -0.43459275\n",
      " -0.37458408 -0.37492746 -0.3408367  -0.5137265  -0.29734734 -0.4651974\n",
      " -0.33712494 -0.55434966 -0.3108238  -0.4165793  -0.4315658  -0.48494196\n",
      " -0.36247966 -0.36582547 -0.19567613 -0.34170026 -0.44047916 -0.2911272\n",
      " -0.34790656 -0.5099424  -0.3605761  -0.426509   -0.4124347  -0.47769466\n",
      " -0.16384417 -0.24189126 -0.35047582 -0.35371703 -0.2695509  -0.57329136\n",
      " -0.42620805 -0.65672404 -0.5221774  -0.329307   -0.31935203 -0.3493078\n",
      " -0.44327363 -0.48174685 -0.49697897 -0.5508511  -0.5072185  -0.24502137\n",
      " -0.39196748 -0.4915042  -0.31089425 -0.40454942 -0.37637407 -0.34803843\n",
      " -0.35864908 -0.47976834 -0.33156818 -0.5761748  -0.33896333 -0.23241799\n",
      " -0.4505923  -0.40767878 -0.41681543 -0.35232186 -0.3870611  -0.33150688\n",
      " -0.41199458 -0.5693161  -0.5573538  -0.48218212 -0.3533362  -0.38253227\n",
      " -0.50474924 -0.4869705  -0.36158675 -0.3773309  -0.37002102 -0.4474187\n",
      " -0.45184067 -0.4074853  -0.4435616  -0.5469824  -0.35114795 -0.49558905\n",
      " -0.48135766 -0.41718665 -0.3598118  -0.49267426 -0.50243187 -0.25435176\n",
      " -0.4308177  -0.43099537 -0.45483738 -0.5528253  -0.44408888 -0.39793605\n",
      " -0.54268837 -0.26346523 -0.3585134  -0.4404091  -0.5595654  -0.29280224\n",
      " -0.49667433 -0.44526848 -0.38623333 -0.3864681  -0.44907066 -0.33782178\n",
      " -0.4296932  -0.474254   -0.37898362 -0.38333377 -0.49859622 -0.49161115\n",
      " -0.45186058 -0.36205134 -0.49930573 -0.29240596 -0.5141494  -0.42489848\n",
      " -0.53453207 -0.2565727  -0.4631551  -0.49464914 -0.49854463 -0.49766132\n",
      " -0.38678813 -0.44368538 -0.23540731 -0.40898412 -0.25117123 -0.38031128\n",
      " -0.328389   -0.28322464 -0.38476345 -0.21787843 -0.33517864 -0.37023774\n",
      " -0.39747718 -0.474761   -0.28537515 -0.35573012 -0.3876825  -0.47356114\n",
      " -0.2987993  -0.4973047  -0.3996687  -0.42656606 -0.43243608 -0.42182392\n",
      " -0.49955523 -0.4682077  -0.18478759 -0.43748942 -0.38953015 -0.3714517\n",
      " -0.47343957 -0.3627483  -0.35319594 -0.4111206  -0.40424764 -0.32697967\n",
      " -0.34498155 -0.35476005 -0.46761465 -0.43432337 -0.40633318 -0.5601965\n",
      " -0.45334128 -0.492429   -0.3657211  -0.3992759  -0.26250145 -0.19293486\n",
      " -0.2942701  -0.48460624 -0.35251212 -0.31386065 -0.31615785 -0.40126196\n",
      " -0.38576284 -0.35619223 -0.4844657  -0.40053648 -0.4134158  -0.3715954\n",
      " -0.4068309  -0.35436976 -0.2895418  -0.49399188 -0.48149616 -0.56675524\n",
      " -0.48244503 -0.5051766  -0.35969546 -0.48938885 -0.35595673 -0.40474996\n",
      " -0.36213055 -0.41242832 -0.41396978 -0.4161062  -0.5278796  -0.47733778\n",
      " -0.4354582  -0.38155118 -0.44777483 -0.31409737 -0.410751   -0.28726178\n",
      " -0.4007992  -0.47361895 -0.47600257 -0.39853212 -0.4614422  -0.38913053\n",
      " -0.2595881  -0.39684084 -0.37154976 -0.22135536 -0.32567587 -0.27808198\n",
      " -0.5302952  -0.35798258 -0.62992364 -0.41921583 -0.4482198  -0.5192323\n",
      " -0.40001333 -0.49002555 -0.46397597 -0.47489548 -0.4908776  -0.4419119\n",
      " -0.4684485  -0.30892476 -0.3864717  -0.10798017 -0.3115597  -0.5190719\n",
      " -0.37922668 -0.41598308 -0.44483426 -0.43693534 -0.383438   -0.3137838\n",
      " -0.46851018 -0.4178952  -0.32461253 -0.4235946  -0.4646705  -0.50865465\n",
      " -0.4052555  -0.43861777 -0.2420209  -0.1494185  -0.5080776  -0.37829953\n",
      " -0.30254364 -0.4828397  -0.5884878  -0.35522404 -0.28374383 -0.3152157\n",
      " -0.0608643  -0.30703005 -0.40405062 -0.39501998 -0.35594192 -0.31224555\n",
      " -0.5563142  -0.38234442 -0.300249   -0.5347258  -0.41128433 -0.32643157\n",
      " -0.44692424 -0.4855441  -0.4095994  -0.37585133 -0.47168988 -0.39845166\n",
      " -0.43798763 -0.47578523 -0.54755735 -0.32869294 -0.41482994 -0.39103076\n",
      " -0.3397423  -0.5429914  -0.4006496  -0.4456991  -0.435015   -0.42569867\n",
      " -0.5789882  -0.45325905 -0.3173653  -0.23279426 -0.4042651  -0.3718124\n",
      " -0.2788553  -0.3619122  -0.3685535  -0.28447813 -0.36186156 -0.42396578\n",
      " -0.44204494 -0.4833887  -0.33341387 -0.3576249  -0.42967987 -0.47227722\n",
      " -0.46041155 -0.5043257  -0.4727797  -0.26233944 -0.3762212  -0.47548765\n",
      " -0.40030006 -0.36700228 -0.38849157 -0.32276776 -0.46614218 -0.49892882\n",
      " -0.32804352 -0.45130748 -0.38346237 -0.5263639  -0.31687728 -0.25969034\n",
      " -0.44503    -0.46624273 -0.45695028 -0.06644553 -0.529165   -0.48707214\n",
      " -0.42083386 -0.4315443  -0.48181498 -0.52236867 -0.4238536  -0.40643102\n",
      " -0.4483064  -0.32692966 -0.3353396  -0.40196922 -0.4660592  -0.37855414\n",
      " -0.5753887  -0.41530567 -0.35604057 -0.47257137 -0.5006023  -0.3395873\n",
      " -0.26868212 -0.50984997 -0.43255946 -0.5317418  -0.45052463 -0.45478356\n",
      " -0.51213276 -0.4795206  -0.4499379  -0.30326086 -0.46271524 -0.29375345\n",
      " -0.3229871  -0.32078955 -0.4301875  -0.42377242 -0.324404   -0.43457058\n",
      " -0.49968722 -0.53029    -0.39799652 -0.4516649  -0.35843846 -0.41016865\n",
      " -0.33057132 -0.3805199  -0.32478723 -0.42422232 -0.45048246 -0.29618797\n",
      " -0.44242305 -0.45113346 -0.3481904  -0.5288397  -0.50509083 -0.38994712\n",
      " -0.29791874 -0.49524122 -0.43724117 -0.22742778 -0.43467045 -0.4565351\n",
      " -0.5521954  -0.5102962  -0.32340556 -0.5402644  -0.38654077 -0.43861684\n",
      " -0.38980427 -0.52408296 -0.49371275 -0.07402004 -0.4733338  -0.3011388\n",
      " -0.45144275 -0.4767816  -0.32353216 -0.5214449  -0.3109174  -0.37485984\n",
      " -0.52998275 -0.41600865 -0.4940462  -0.36927035 -0.43512398 -0.3029667\n",
      " -0.52154785 -0.5125134  -0.16999577 -0.5077903  -0.40616223 -0.42630953\n",
      " -0.388791   -0.3984859  -0.47203216 -0.4996207  -0.41346437 -0.4584091\n",
      " -0.48720554 -0.38218516 -0.40523311 -0.44606957 -0.45770746 -0.3276092\n",
      " -0.44723633 -0.40516558 -0.4403244  -0.38125485 -0.4314017  -0.30423006\n",
      " -0.45232886 -0.4498657  -0.28356254 -0.5099254  -0.41885972 -0.37090504\n",
      " -0.37346974 -0.33640006 -0.2807521  -0.37949958 -0.4300472  -0.5053558\n",
      " -0.40326697 -0.36601353 -0.41788682 -0.48080063 -0.49241805 -0.49672315\n",
      " -0.37790045 -0.4268779  -0.37415698 -0.40342736 -0.4590292  -0.38266435\n",
      " -0.37904742 -0.48352686 -0.4901454  -0.46825114 -0.35621208 -0.65666765\n",
      " -0.50619036 -0.538479   -0.5189301  -0.4483571  -0.35672206 -0.2953608\n",
      " -0.27235982 -0.44234493 -0.33749    -0.41188705 -0.47760996 -0.44014668\n",
      " -0.4234475  -0.55100036 -0.48128515 -0.42058113 -0.25216168 -0.5230675\n",
      " -0.45131847 -0.51054275]\n",
      "DenseRes0.FC0.weight DenseRes0_w0\n",
      "(2359296,)\n",
      "torch.Size([1536, 1536])\n",
      "Parameter containing:\n",
      "tensor([[-0.0775, -0.0064, -0.0510,  ..., -0.0043, -0.0301, -0.0015],\n",
      "        [ 0.0517, -0.0349,  0.0411,  ..., -0.0302,  0.0051,  0.0243],\n",
      "        [ 0.0110,  0.0172, -0.0879,  ...,  0.0141,  0.0051, -0.0188],\n",
      "        ...,\n",
      "        [ 0.0063, -0.0092, -0.0204,  ..., -0.1305,  0.0003, -0.0358],\n",
      "        [ 0.0056, -0.0841,  0.0102,  ...,  0.0606, -0.1223, -0.0771],\n",
      "        [ 0.0249,  0.0041, -0.0096,  ...,  0.0202, -0.1051, -0.1577]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "[-0.0775284  -0.00640686 -0.05098232 ...  0.02024142 -0.10511503\n",
      " -0.15766998]\n",
      "DenseRes0.FC0.bias DenseRes0_b0\n",
      "(1536,)\n",
      "torch.Size([1536])\n",
      "Parameter containing:\n",
      "tensor([-0.0224, -0.0647, -0.0145,  ..., -0.0282,  0.0014,  0.0359],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "[-0.0223931  -0.06468304 -0.01450021 ... -0.02817546  0.0014137\n",
      "  0.03591918]\n",
      "DenseRes0.FC1.weight DenseRes1_w0\n",
      "(2359296,)\n",
      "torch.Size([1536, 1536])\n",
      "Parameter containing:\n",
      "tensor([[-0.0737, -0.0338,  0.0311,  ..., -0.0242, -0.0117,  0.0082],\n",
      "        [-0.0054, -0.0631,  0.0070,  ...,  0.0044,  0.0360, -0.0319],\n",
      "        [-0.0416, -0.0213, -0.0777,  ..., -0.0098,  0.0285, -0.0088],\n",
      "        ...,\n",
      "        [-0.0117, -0.0851, -0.0075,  ..., -0.0698, -0.0015, -0.0039],\n",
      "        [ 0.0165, -0.0517, -0.0157,  ..., -0.0226, -0.1172,  0.0326],\n",
      "        [-0.0314,  0.0082, -0.0248,  ..., -0.0184,  0.0091, -0.0266]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "[-0.07365504 -0.0338111   0.03109055 ... -0.01836685  0.0090701\n",
      " -0.02664338]\n",
      "DenseRes0.FC1.bias DenseRes1_b0\n",
      "(1536,)\n",
      "torch.Size([1536])\n",
      "Parameter containing:\n",
      "tensor([-0.0423, -0.0612, -0.0310,  ..., -0.1044, -0.0595, -0.1060],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "[-0.04230716 -0.06122421 -0.03097558 ... -0.10439949 -0.05951711\n",
      " -0.106039  ]\n",
      "DenseRes1.FC0.weight DenseRes2_w0\n",
      "(2359296,)\n",
      "torch.Size([1536, 1536])\n",
      "Parameter containing:\n",
      "tensor([[-0.0264, -0.0096, -0.0427,  ...,  0.0178,  0.0038,  0.0532],\n",
      "        [ 0.0268, -0.0274,  0.0062,  ...,  0.0245,  0.0287, -0.0350],\n",
      "        [-0.0350, -0.0038, -0.0493,  ..., -0.0351,  0.0263,  0.0284],\n",
      "        ...,\n",
      "        [ 0.0236, -0.0454,  0.0349,  ..., -0.0291, -0.0442,  0.0035],\n",
      "        [-0.0005,  0.0202, -0.0289,  ..., -0.0240, -0.0661, -0.0209],\n",
      "        [-0.0266, -0.0543, -0.0307,  ..., -0.0123, -0.0232, -0.0683]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "[-0.02636723 -0.00955026 -0.04266318 ... -0.0122555  -0.02320482\n",
      " -0.06829059]\n",
      "DenseRes1.FC0.bias DenseRes2_b0\n",
      "(1536,)\n",
      "torch.Size([1536])\n",
      "Parameter containing:\n",
      "tensor([ 0.0944,  0.0378, -0.0911,  ...,  0.0016, -0.0856, -0.1828],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "[ 0.09438591  0.03778703 -0.09112459 ...  0.00160412 -0.08558722\n",
      " -0.18276674]\n",
      "DenseRes1.FC1.weight DenseRes3_w0\n",
      "(2359296,)\n",
      "torch.Size([1536, 1536])\n",
      "Parameter containing:\n",
      "tensor([[-0.1388,  0.0460,  0.0260,  ..., -0.0190,  0.0269,  0.0267],\n",
      "        [ 0.0064, -0.0743,  0.0202,  ...,  0.0291,  0.0511,  0.0196],\n",
      "        [ 0.0369,  0.0014, -0.0146,  ...,  0.0172,  0.0224,  0.0016],\n",
      "        ...,\n",
      "        [ 0.0277, -0.0417,  0.0457,  ..., -0.0415,  0.0120, -0.0253],\n",
      "        [-0.0237,  0.0103,  0.0053,  ..., -0.0260, -0.1017,  0.0271],\n",
      "        [ 0.0041,  0.0202, -0.0264,  ...,  0.0091, -0.0036, -0.1691]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "[-0.13882712  0.04599845  0.02603413 ...  0.00905659 -0.00356226\n",
      " -0.16911334]\n",
      "DenseRes1.FC1.bias DenseRes3_b0\n",
      "(1536,)\n",
      "torch.Size([1536])\n",
      "Parameter containing:\n",
      "tensor([-0.1066, -0.1076, -0.0680,  ...,  0.0069,  0.0385,  0.1092],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "[-0.10656974 -0.10758325 -0.06795736 ...  0.00688065  0.03851226\n",
      "  0.10915776]\n",
      "Decoder.weight Decoder_w0\n",
      "(328704,)\n",
      "torch.Size([214, 1536])\n",
      "Parameter containing:\n",
      "tensor([[ 3.0861e-04,  1.1433e-04, -3.4010e-04,  ..., -1.1636e-04,\n",
      "          3.1003e-04, -2.4568e-04],\n",
      "        [ 2.9357e-04,  1.1943e-04, -3.0039e-04,  ..., -1.1222e-04,\n",
      "          2.8861e-04, -2.4852e-04],\n",
      "        [ 2.9777e-04,  1.1821e-04, -3.1073e-04,  ..., -1.1300e-04,\n",
      "          2.9425e-04, -2.4835e-04],\n",
      "        ...,\n",
      "        [-1.0075e-02,  1.5461e-02,  1.8501e-03,  ...,  2.8796e-03,\n",
      "          6.4911e-03, -6.0163e-03],\n",
      "        [ 8.9461e-03,  4.2349e-03,  1.8365e-03,  ...,  5.4857e-03,\n",
      "          5.1442e-03, -8.6557e-03],\n",
      "        [ 5.4899e-03,  3.7188e-03,  1.2551e-03,  ...,  5.4487e-03,\n",
      "          1.2318e-03,  1.6582e-05]], device='cuda:0', requires_grad=True)\n",
      "[ 3.08606919e-04  1.14326824e-04 -3.40101658e-04 ...  5.44869900e-03\n",
      "  1.23178470e-03  1.65823603e-05]\n",
      "Decoder.bias Decoder_b0\n",
      "(214,)\n",
      "torch.Size([214])\n",
      "Parameter containing:\n",
      "tensor([ 9.6603e-04,  9.7491e-04,  9.7371e-04,  9.7539e-04,  9.7093e-04,\n",
      "         9.7559e-04, -1.1627e-02, -6.4976e-03, -1.0413e-02,  1.3800e-03,\n",
      "        -7.0803e-03, -1.7432e-02,  5.6770e-03,  4.9800e-03, -1.5724e-02,\n",
      "         9.7479e-04, -9.7592e-04, -9.7483e-04, -1.5267e-02,  6.0294e-04,\n",
      "         9.4852e-03, -1.3901e-02,  4.1038e-04,  2.6089e-03, -9.7226e-04,\n",
      "         9.7557e-04,  9.6355e-04, -1.1959e-02, -2.4543e-03,  3.1645e-03,\n",
      "        -2.7604e-03, -4.7162e-04, -7.6698e-03,  9.8776e-04,  9.7278e-04,\n",
      "        -9.7402e-04, -1.3682e-02, -5.3137e-03,  4.7898e-04, -5.0328e-03,\n",
      "        -4.6622e-03, -7.9847e-03,  9.6916e-04, -9.7312e-04,  9.7553e-04,\n",
      "        -1.0558e-02, -1.0322e-03, -3.8704e-03, -5.4300e-03, -1.4223e-04,\n",
      "        -7.8234e-03,  2.7315e-03,  1.5021e-02, -1.5554e-02, -9.4068e-03,\n",
      "         1.0412e-03, -3.8226e-03, -1.1028e-03,  5.1591e-04, -1.3455e-02,\n",
      "        -2.5723e-03, -3.9613e-03, -1.3827e-02, -2.4042e-03, -1.2752e-03,\n",
      "        -1.4968e-02,  9.7393e-04,  9.7391e-04,  9.7559e-04,  9.7294e-04,\n",
      "        -6.7830e-04,  9.7742e-04,  9.7529e-04,  9.7679e-04, -1.0259e-03,\n",
      "         9.7731e-04, -9.7514e-04, -9.8959e-04, -9.7099e-04,  9.7295e-04,\n",
      "         9.4509e-04,  9.7305e-04, -9.7589e-04, -9.7036e-04,  9.6391e-04,\n",
      "         9.7361e-04,  9.7910e-04,  9.6695e-04,  9.6712e-04,  9.7256e-04,\n",
      "        -9.7522e-04,  7.3511e-04, -9.8999e-04,  9.7643e-04,  9.8909e-04,\n",
      "        -9.6605e-04, -9.7052e-04, -9.6615e-04,  9.7545e-04, -3.8939e-03,\n",
      "         8.7489e-03, -8.4047e-04, -7.7556e-03,  1.3161e-02, -7.4601e-03,\n",
      "        -1.3612e-02,  1.3061e-02,  2.2883e-02, -1.4944e-02,  1.4211e-02,\n",
      "        -2.6326e-03,  1.0056e-02, -1.2167e-02,  1.3129e-02, -1.2800e-02,\n",
      "         1.3146e-02, -1.4862e-03,  2.3418e-02, -1.7318e-02,  1.6237e-02,\n",
      "        -9.8280e-03, -1.4988e-02, -1.4039e-02,  2.3681e-03, -7.7658e-03,\n",
      "        -8.2768e-03,  1.8261e-03, -6.6253e-03, -9.3640e-03, -4.5223e-03,\n",
      "        -4.4037e-04,  1.1923e-02,  3.3373e-03, -5.4871e-03, -8.4264e-03,\n",
      "        -1.8458e-02, -6.7889e-03,  3.3110e-03, -4.0412e-03, -3.6567e-03,\n",
      "        -1.2408e-02,  4.1589e-03, -6.2174e-03, -9.1203e-03,  4.2055e-03,\n",
      "        -5.5012e-03, -9.3011e-03, -1.7515e-02, -8.1383e-03, -1.0874e-03,\n",
      "         9.1161e-03, -3.1646e-03,  7.6547e-04, -2.4463e-02, -9.7476e-03,\n",
      "        -4.1267e-03, -1.4997e-03,  2.9522e-04, -1.3327e-02,  5.3911e-03,\n",
      "        -6.4508e-03, -1.8259e-02,  5.5589e-03, -4.7879e-03, -1.7375e-02,\n",
      "        -1.5943e-02, -7.3264e-03,  1.4223e-03,  5.7872e-03, -4.5253e-03,\n",
      "        -1.7090e-02, -2.3998e-02, -8.7034e-03, -4.2154e-04,  3.5821e-03,\n",
      "         6.9185e-03, -8.0447e-03,  4.0517e-03,  2.9190e-04, -2.3095e-02,\n",
      "         5.7134e-03,  1.3855e-03, -1.6857e-02, -1.0624e-02, -8.0072e-03,\n",
      "         1.8846e-03,  4.1223e-03,  1.1915e-03, -2.2278e-02, -2.0170e-02,\n",
      "        -1.1027e-02,  1.5862e-04,  9.7374e-04, -1.8136e-03,  6.8011e-03,\n",
      "        -1.2078e-02, -3.2302e-02, -1.4836e-02, -2.3316e-02, -2.7173e-02,\n",
      "        -5.8858e-03, -2.1108e-02, -2.8265e-02, -4.0770e-03, -7.4346e-03,\n",
      "        -2.0806e-02, -2.6210e-03,  8.9441e-05, -7.8058e-03, -2.7251e-02,\n",
      "        -2.4184e-02, -1.9776e-02, -1.5242e-02, -8.6231e-03], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "[ 9.66034131e-04  9.74905270e-04  9.73705144e-04  9.75394039e-04\n",
      "  9.70926543e-04  9.75592935e-04 -1.16274739e-02 -6.49763364e-03\n",
      " -1.04131578e-02  1.38001773e-03 -7.08034635e-03 -1.74323656e-02\n",
      "  5.67704393e-03  4.97996202e-03 -1.57237928e-02  9.74790833e-04\n",
      " -9.75919422e-04 -9.74825118e-04 -1.52666606e-02  6.02936430e-04\n",
      "  9.48523637e-03 -1.39010884e-02  4.10376844e-04  2.60888692e-03\n",
      " -9.72261303e-04  9.75570583e-04  9.63552913e-04 -1.19591439e-02\n",
      " -2.45433697e-03  3.16445855e-03 -2.76036444e-03 -4.71615524e-04\n",
      " -7.66984327e-03  9.87764215e-04  9.72779468e-04 -9.74022085e-04\n",
      " -1.36818541e-02 -5.31367492e-03  4.78979229e-04 -5.03275357e-03\n",
      " -4.66222782e-03 -7.98466522e-03  9.69163142e-04 -9.73120390e-04\n",
      "  9.75533854e-04 -1.05582457e-02 -1.03220600e-03 -3.87038849e-03\n",
      " -5.42998547e-03 -1.42225050e-04 -7.82336015e-03  2.73146364e-03\n",
      "  1.50210038e-02 -1.55537901e-02 -9.40679386e-03  1.04116951e-03\n",
      " -3.82256531e-03 -1.10284355e-03  5.15914930e-04 -1.34546692e-02\n",
      " -2.57232646e-03 -3.96131258e-03 -1.38266273e-02 -2.40417966e-03\n",
      " -1.27523195e-03 -1.49679938e-02  9.73933260e-04  9.73905786e-04\n",
      "  9.75587114e-04  9.72941751e-04 -6.78301731e-04  9.77415708e-04\n",
      "  9.75291012e-04  9.76792886e-04 -1.02592877e-03  9.77311516e-04\n",
      " -9.75137518e-04 -9.89587745e-04 -9.70994646e-04  9.72953159e-04\n",
      "  9.45090840e-04  9.73050541e-04 -9.75892472e-04 -9.70357214e-04\n",
      "  9.63908329e-04  9.73610790e-04  9.79100354e-04  9.66952648e-04\n",
      "  9.67119006e-04  9.72563750e-04 -9.75221861e-04  7.35105132e-04\n",
      " -9.89986584e-04  9.76425479e-04  9.89090535e-04 -9.66049032e-04\n",
      " -9.70520952e-04 -9.66147229e-04  9.75453935e-04 -3.89390253e-03\n",
      "  8.74893460e-03 -8.40469496e-04 -7.75561761e-03  1.31607633e-02\n",
      " -7.46012852e-03 -1.36123048e-02  1.30611090e-02  2.28830464e-02\n",
      " -1.49439638e-02  1.42112896e-02 -2.63259071e-03  1.00559462e-02\n",
      " -1.21667944e-02  1.31286895e-02 -1.27996653e-02  1.31455464e-02\n",
      " -1.48623087e-03  2.34183390e-02 -1.73183586e-02  1.62366480e-02\n",
      " -9.82803665e-03 -1.49882156e-02 -1.40391653e-02  2.36812746e-03\n",
      " -7.76577042e-03 -8.27681832e-03  1.82610122e-03 -6.62532588e-03\n",
      " -9.36398190e-03 -4.52232966e-03 -4.40374075e-04  1.19225094e-02\n",
      "  3.33732856e-03 -5.48708346e-03 -8.42640083e-03 -1.84579808e-02\n",
      " -6.78892899e-03  3.31098842e-03 -4.04124893e-03 -3.65673704e-03\n",
      " -1.24083096e-02  4.15892247e-03 -6.21737773e-03 -9.12029389e-03\n",
      "  4.20553098e-03 -5.50116412e-03 -9.30108782e-03 -1.75151750e-02\n",
      " -8.13825708e-03 -1.08738185e-03  9.11612064e-03 -3.16463248e-03\n",
      "  7.65473291e-04 -2.44631879e-02 -9.74764116e-03 -4.12671594e-03\n",
      " -1.49973144e-03  2.95223988e-04 -1.33267595e-02  5.39105060e-03\n",
      " -6.45083375e-03 -1.82586964e-02  5.55887166e-03 -4.78790002e-03\n",
      " -1.73754282e-02 -1.59426425e-02 -7.32641714e-03  1.42228696e-03\n",
      "  5.78718493e-03 -4.52534482e-03 -1.70903988e-02 -2.39981003e-02\n",
      " -8.70340690e-03 -4.21537436e-04  3.58209247e-03  6.91848109e-03\n",
      " -8.04474764e-03  4.05169884e-03  2.91902572e-04 -2.30953656e-02\n",
      "  5.71343116e-03  1.38554443e-03 -1.68574415e-02 -1.06239440e-02\n",
      " -8.00715666e-03  1.88459270e-03  4.12228098e-03  1.19150546e-03\n",
      " -2.22777035e-02 -2.01702248e-02 -1.10271173e-02  1.58618699e-04\n",
      "  9.73742106e-04 -1.81364117e-03  6.80112885e-03 -1.20779118e-02\n",
      " -3.23015712e-02 -1.48360571e-02 -2.33159978e-02 -2.71732472e-02\n",
      " -5.88577567e-03 -2.11078338e-02 -2.82648541e-02 -4.07700893e-03\n",
      " -7.43456185e-03 -2.08060499e-02 -2.62098107e-03  8.94414552e-05\n",
      " -7.80583825e-03 -2.72514690e-02 -2.41836738e-02 -1.97762325e-02\n",
      " -1.52424192e-02 -8.62312317e-03]\n"
     ]
    }
   ],
   "source": [
    "getbinname = {\n",
    "    \"Decoder.weight\": \"Decoder_w0\",\n",
    "    \"Decoder.bias\": \"Decoder_b0\",\n",
    "    \"Encoder0.weight\": \"Encoder0_w0\",\n",
    "    \"Encoder0.bias\": \"Encoder0_b0\",\n",
    "    \"Encoder1.weight\": \"Encoder1_w0\",\n",
    "    \"Encoder1.bias\": \"Encoder1_b0\",\n",
    "    \"Encoder2.weight\": \"Encoder2_w0\",\n",
    "    \"Encoder2.bias\": \"Encoder2_b0\",\n",
    "    \"DenseRes0.FC0.weight\": \"DenseRes0_w0\",\n",
    "    \"DenseRes0.FC0.bias\": \"DenseRes0_b0\",\n",
    "    \"DenseRes0.FC1.weight\": \"DenseRes1_w0\",\n",
    "    \"DenseRes0.FC1.bias\": \"DenseRes1_b0\",\n",
    "    \"DenseRes1.FC0.weight\": \"DenseRes2_w0\",\n",
    "    \"DenseRes1.FC0.bias\": \"DenseRes2_b0\",\n",
    "    \"DenseRes1.FC1.weight\": \"DenseRes3_w0\",\n",
    "    \"DenseRes1.FC1.bias\": \"DenseRes3_b0\",\n",
    "\n",
    "}\n",
    "# print(os.getcwd())\n",
    "#\n",
    "with torch.no_grad():\n",
    "    for name, param in model.named_parameters():\n",
    "        print(name, getbinname[name])\n",
    "        # fp = f\"data/ManipNetBIN/{getbinname[name]}.bin\"\n",
    "        fp = f\"{getbinname[name]}.bin\"\n",
    "        savednp = np.fromfile(fp, dtype=np.float32)\n",
    "        print(savednp.shape)\n",
    "        # savedweight = torch.from_numpy(savednp)\n",
    "        shape_ = param.shape\n",
    "        print(shape_)\n",
    "\n",
    "        # print(shape_, shape_[0], shape_[1])\n",
    "        param.copy_(torch.from_numpy(savednp.reshape(shape_, order=\"C\"))) \n",
    "        # param.copy_(torch.from_numpy(savednp.reshape(shape_, order=\"F\"))) \n",
    "        print(param)\n",
    "        print(savednp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder0.weight\n",
      "torch.Size([512, 192]) Encoder0.weight Encoder0_w0\n",
      "Encoder0.bias\n",
      "torch.Size([512]) Encoder0.bias Encoder0_b0\n",
      "Encoder1.weight\n",
      "torch.Size([512, 336]) Encoder1.weight Encoder1_w0\n",
      "Encoder1.bias\n",
      "torch.Size([512]) Encoder1.bias Encoder1_b0\n",
      "Encoder2.weight\n",
      "torch.Size([512, 1822]) Encoder2.weight Encoder2_w0\n",
      "Encoder2.bias\n",
      "torch.Size([512]) Encoder2.bias Encoder2_b0\n",
      "DenseRes0.FC0.weight\n",
      "DenseRes0.FC0.bias\n",
      "DenseRes0.FC1.weight\n",
      "DenseRes0.FC1.bias\n",
      "DenseRes1.FC0.weight\n",
      "DenseRes1.FC0.bias\n",
      "DenseRes1.FC1.weight\n",
      "DenseRes1.FC1.bias\n",
      "Decoder.weight\n",
      "torch.Size([214, 1536]) Decoder.weight Decoder_w0\n",
      "Decoder.bias\n",
      "torch.Size([214]) Decoder.bias Decoder_b0\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for name, param in model.named_parameters():\n",
    "        print(name)\n",
    "        for k, v in bindata.items():\n",
    "            for i in range(len(net_name_list)):\n",
    "                # print(net_name_list[i][0], name)\n",
    "                if name == net_name_list[i][0] and k == net_name_list[i][1]:\n",
    "                    print(param.shape, name, k)\n",
    "                    rephape_weight = torch.reshape(torch.from_numpy(bindata[net_name_list[i][1]]), net_name_list[i][2])\n",
    "                    param.copy_(rephape_weight)\n",
    "        for i in range(len(dense_weight_list)):\n",
    "            if name == dense_weight_list[i][0]:\n",
    "                param.copy_(torch.reshape(torch.from_numpy(bindata[dense_weight_list[i][1]]), [1536, 1536]))\n",
    "            if name == dense_bias_list[i][0]:\n",
    "                param.copy_(torch.reshape(torch.from_numpy(bindata[dense_bias_list[i][1]]), [1536]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder0.weight Parameter containing:\n",
      "tensor([[-0.0751, -0.0893,  0.0815,  ..., -0.0023,  0.0108,  0.0553],\n",
      "        [-0.0455,  0.0524, -0.0306,  ..., -0.0354, -0.0475, -0.0086],\n",
      "        [-0.0329,  0.0703, -0.0476,  ...,  0.0040,  0.0093, -0.0899],\n",
      "        ...,\n",
      "        [ 0.0060,  0.0643, -0.0282,  ...,  0.0184,  0.0319, -0.0253],\n",
      "        [-0.0182,  0.0615,  0.0271,  ...,  0.0675, -0.0561, -0.0581],\n",
      "        [-0.0358,  0.0303,  0.0649,  ..., -0.0089,  0.0761, -0.2177]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Encoder0.bias "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([-0.1640, -0.3014, -0.1578, -0.0544, -0.2442, -0.4230, -0.0730, -0.1455,\n",
      "        -0.1958, -0.2338, -0.1662, -0.1184, -0.1595, -0.1054, -0.2449, -0.2744,\n",
      "         0.3854, -0.2818,  0.5248, -0.0677, -0.1090, -0.1178, -0.2102,  0.2377,\n",
      "         0.4789,  0.1886, -0.2035, -0.2202, -0.3630, -0.1779, -0.1787, -0.0984,\n",
      "        -0.1555, -0.1249, -0.1549, -0.3549, -0.1839, -0.1523, -0.1989, -0.1677,\n",
      "        -0.0936, -0.1551, -0.2757, -0.1913, -0.2227, -0.1848, -0.1450, -0.2108,\n",
      "         0.2748, -0.3838, -0.1906, -0.0878, -0.3309, -0.2950, -0.1489, -0.1433,\n",
      "        -0.0678, -0.0986, -0.3596, -0.3442, -0.1202, -0.1292, -0.2998, -0.3798,\n",
      "        -0.5403, -0.2838,  0.3112, -0.2935, -0.3343, -0.2571, -0.0496, -0.1544,\n",
      "        -0.1000, -0.3391, -0.3416, -0.2133, -0.2621, -0.3774, -0.1920, -0.1826,\n",
      "        -0.2558, -0.2018, -0.3717,  0.1867, -0.1788, -0.1508, -0.3846, -0.2013,\n",
      "        -0.1610, -0.1289,  0.4089, -0.1777, -0.3522, -0.1177,  0.5571, -0.3284,\n",
      "         0.5220, -0.1085, -0.1776, -0.1438, -0.2332, -0.1896, -0.2541, -0.2952,\n",
      "        -0.2068, -0.1214, -0.1102, -0.2018, -0.2112, -0.1603, -0.1218, -0.0671,\n",
      "        -0.2647,  0.1929, -0.2281, -0.1906, -0.1397, -0.1677, -0.3873, -0.1258,\n",
      "        -0.3395, -0.1583, -0.2389, -0.2625, -0.2412, -0.1295, -0.1360, -0.0635,\n",
      "        -0.1498, -0.1031, -0.1074, -0.2727, -0.1188, -0.1324, -0.2818, -0.0617,\n",
      "        -0.4059, -0.1478, -0.0956, -0.1221, -0.1219, -0.1803, -0.1824, -0.1016,\n",
      "         0.3265, -0.3195, -0.5142, -0.1727, -0.2644, -0.1785, -0.1639, -0.1529,\n",
      "        -0.1514, -0.2023,  0.1132, -0.1648, -0.2075, -0.1424, -0.1185, -0.3189,\n",
      "        -0.2751, -0.1782, -0.0904, -0.3374, -0.1831, -0.1108, -0.1497, -0.0749,\n",
      "        -0.2373,  0.5636, -0.1655, -0.1761, -0.1318, -0.1311, -0.1226, -0.3297,\n",
      "        -0.1105,  0.0322, -0.1865, -0.1200, -0.1420, -0.0131, -0.1520, -0.3672,\n",
      "        -0.2242, -0.1832, -0.1601, -0.4223, -0.1355, -0.1323,  0.1483, -0.0810,\n",
      "        -0.1676, -0.0816, -0.1722, -0.2645, -0.1598, -0.1274, -0.0785, -0.2377,\n",
      "        -0.1239,  0.3252, -0.1377, -0.0333, -0.0974, -0.2369, -0.4341, -0.1264,\n",
      "        -0.1963, -0.0879, -0.0705,  0.2595, -0.1618, -0.1682, -0.1282, -0.0621,\n",
      "        -0.1085, -0.1299, -0.1556,  0.6140, -0.3024, -0.1786,  0.2916, -0.1016,\n",
      "        -0.1383, -0.3019, -0.2828, -0.1330, -0.3842, -0.2050, -0.1671, -0.1569,\n",
      "        -0.1753, -0.2015, -0.0839, -0.3690, -0.5441, -0.3087, -0.2941, -0.2598,\n",
      "        -0.1406, -0.2503, -0.2124, -0.1061, -0.1275, -0.2483, -0.1663,  0.0573,\n",
      "        -0.1005, -0.2936, -0.2052, -0.2906, -0.1547,  0.1829, -0.1515, -0.2765,\n",
      "         0.2386, -0.1973,  0.3433, -0.2286, -0.2405, -0.1087, -0.1455, -0.1787,\n",
      "         0.2950, -0.3225, -0.0229, -0.1671, -0.1192,  0.3374,  0.2715, -0.1531,\n",
      "        -0.2548, -0.1279, -0.0361, -0.1904, -0.0837, -0.1230, -0.1748, -0.1108,\n",
      "        -0.1861, -0.2155, -0.3162, -0.2970, -0.2553, -0.2535, -0.2059,  0.3050,\n",
      "         0.2146, -0.2594, -0.1646, -0.1511,  0.0179, -0.2290, -0.1457, -0.0335,\n",
      "        -0.1483, -0.1141, -0.0747,  0.0856, -0.3069, -0.1689, -0.0301, -0.1490,\n",
      "        -0.2194, -0.2017, -0.1323, -0.4129, -0.2350, -0.2978, -0.1162, -0.3352,\n",
      "         0.3423, -0.1731, -0.1221, -0.1840, -0.2306, -0.1228, -0.1761, -0.2025,\n",
      "        -0.1910, -0.1375, -0.2802, -0.1821, -0.1476, -0.1194, -0.1308, -0.1568,\n",
      "        -0.2535, -0.2404, -0.3091, -0.1749, -0.1412, -0.1588, -0.1799, -0.1338,\n",
      "        -0.1635, -0.2909, -0.1468,  0.1187, -0.1731, -0.1908, -0.1999, -0.3052,\n",
      "        -0.0985, -0.3249, -0.1970, -0.3925, -0.2470, -0.2235, -0.1803, -0.1424,\n",
      "         0.3621, -0.1608, -0.2231, -0.1787, -0.2517, -0.1474, -0.3742,  0.1497,\n",
      "        -0.1302, -0.1326, -0.1354, -0.2421, -0.2031, -0.2032,  0.0543, -0.2035,\n",
      "        -0.2268, -0.3376, -0.2610, -0.1591, -0.1457, -0.2799, -0.2189, -0.1037,\n",
      "        -0.3752,  0.1436, -0.1019, -0.0661, -0.1344, -0.1216, -0.2023, -0.1113,\n",
      "        -0.1609, -0.3735, -0.1303,  0.1622, -0.2131, -0.2040, -0.1814, -0.1481,\n",
      "        -0.0888, -0.1660, -0.3547,  0.3531, -0.2156, -0.2752, -0.1976, -0.1453,\n",
      "        -0.2382, -0.2779, -0.0908, -0.2122, -0.2339, -0.0859, -0.3178, -0.2652,\n",
      "        -0.3201, -0.1810, -0.3678, -0.1718, -0.2538,  0.1322, -0.3014, -0.1906,\n",
      "        -0.1203, -0.1515, -0.1070, -0.2081, -0.2013, -0.1494, -0.1258, -0.3503,\n",
      "        -0.1271,  0.3416, -0.1348, -0.0689,  0.3573, -0.0028, -0.1705, -0.0953,\n",
      "        -0.1992, -0.1591,  0.0473, -0.1173, -0.2027, -0.1551, -0.1572,  0.2826,\n",
      "        -0.1706, -0.1679, -0.2174, -0.1593,  0.3117, -0.0169, -0.1073, -0.1380,\n",
      "        -0.1600, -0.0988, -0.1966, -0.3516, -0.1780, -0.1800, -0.2236, -0.1692,\n",
      "        -0.1073, -0.1010,  0.5641, -0.1769, -0.0206, -0.2577,  0.1222, -0.1628,\n",
      "        -0.2048, -0.3043, -0.1959, -0.1745, -0.3006, -0.2416,  0.3691, -0.1578,\n",
      "        -0.1074, -0.3076, -0.1280, -0.1990, -0.1435, -0.1874, -0.0948, -0.2300,\n",
      "        -0.1866, -0.2158, -0.0901, -0.1400,  0.0138,  0.3266, -0.0840, -0.3275,\n",
      "        -0.2286, -0.2583, -0.2955, -0.1522,  0.4923,  0.3641, -0.0486, -0.1660,\n",
      "         0.4059,  0.1804, -0.2585, -0.2407, -0.1350,  0.0404, -0.1234, -0.0744,\n",
      "        -0.1041, -0.1266,  0.2202,  0.2329, -0.0773, -0.3262, -0.1330, -0.2185],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Encoder1.weight Parameter containing:\n",
      "tensor([[ 0.0151, -0.0454, -0.0045,  ...,  0.0743, -0.0504, -0.0051],\n",
      "        [-0.0205, -0.0371,  0.0407,  ...,  0.0083, -0.0203, -0.0291],\n",
      "        [-0.0443,  0.0076, -0.0095,  ...,  0.1006, -0.0940,  0.0676],\n",
      "        ...,\n",
      "        [-0.0461, -0.0345,  0.0136,  ..., -0.0455, -0.0179, -0.0300],\n",
      "        [ 0.0266,  0.0437,  0.1292,  ...,  0.0437, -0.0176, -0.0445],\n",
      "        [ 0.0074, -0.0037, -0.0335,  ..., -0.0343, -0.0006,  0.0782]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Encoder1.bias Parameter containing:\n",
      "tensor([-0.2406, -0.3507, -0.3916, -0.3398, -0.1664, -0.2168, -0.3187, -0.3043,\n",
      "        -0.2258, -0.2667, -0.2999, -0.2634, -0.2336, -0.1910, -0.1900, -0.2181,\n",
      "        -0.2915, -0.3086, -0.3763, -0.3578, -0.3352, -0.3185, -0.3127, -0.3608,\n",
      "        -0.3058, -0.3229, -0.3290, -0.2595, -0.3054, -0.2928, -0.3379, -0.2918,\n",
      "        -0.3088, -0.3166, -0.2562, -0.2982, -0.2441, -0.2535, -0.2273, -0.2835,\n",
      "        -0.2893, -0.2202, -0.2721, -0.2743, -0.3047, -0.3187, -0.3952, -0.3036,\n",
      "        -0.3886, -0.2557, -0.2231, -0.2757, -0.2194, -0.2295, -0.2829, -0.2976,\n",
      "        -0.2673, -0.2423, -0.3264, -0.2856, -0.3466, -0.2459, -0.2643, -0.2893,\n",
      "        -0.2956, -0.2841, -0.3279, -0.2971, -0.3203, -0.2207, -0.1854, -0.2690,\n",
      "        -0.2631, -0.3303, -0.3290, -0.2560, -0.3221, -0.2649, -0.2291, -0.3176,\n",
      "        -0.3128, -0.2931, -0.3504, -0.2946, -0.2942, -0.2593, -0.2591, -0.4058,\n",
      "        -0.2106, -0.2383, -0.2301, -0.3027, -0.2607, -0.2684, -0.2358, -0.3877,\n",
      "        -0.2492, -0.2645, -0.2877, -0.1611, -0.2774, -0.2394, -0.2964, -0.3116,\n",
      "        -0.3448, -0.3188, -0.2066, -0.2847, -0.2653, -0.2685, -0.2913, -0.2910,\n",
      "        -0.2426, -0.4096, -0.2006, -0.1753, -0.2774, -0.2912, -0.1771, -0.2589,\n",
      "        -0.2575, -0.3169, -0.2161, -0.3139, -0.3249, -0.1956, -0.2385, -0.3595,\n",
      "        -0.2717, -0.3333, -0.1857, -0.2346, -0.3649, -0.2224, -0.1987, -0.2826,\n",
      "        -0.2383, -0.3042, -0.3339, -0.2686, -0.2948, -0.1815, -0.3564, -0.2522,\n",
      "        -0.2459, -0.2977, -0.2945, -0.3080, -0.1768, -0.2161, -0.3401, -0.2408,\n",
      "        -0.3114, -0.2017, -0.2501, -0.2523, -0.3648, -0.3045, -0.3518, -0.2396,\n",
      "        -0.2699, -0.2609, -0.2115, -0.2208, -0.2989, -0.3622, -0.3623, -0.3209,\n",
      "        -0.2514, -0.2675, -0.3922, -0.2846, -0.3482, -0.2488, -0.2901, -0.3162,\n",
      "        -0.3048, -0.2571, -0.2829, -0.3231, -0.2369, -0.2796, -0.2078, -0.2013,\n",
      "        -0.3201, -0.2192, -0.2517, -0.2549, -0.2820, -0.2952, -0.2775, -0.3310,\n",
      "        -0.2835, -0.2793, -0.2575, -0.2253, -0.1165, -0.2508, -0.2040, -0.2104,\n",
      "        -0.2874, -0.3231, -0.2870, -0.2359, -0.1772, -0.2084, -0.1688, -0.3293,\n",
      "        -0.2962, -0.2373, -0.3500, -0.2376, -0.2386, -0.2387, -0.3689, -0.3092,\n",
      "        -0.3319, -0.2399, -0.3394, -0.2648, -0.2687, -0.3430, -0.3139, -0.2759,\n",
      "        -0.3031, -0.2419, -0.2509, -0.2833, -0.2058, -0.2706, -0.3415, -0.2962,\n",
      "        -0.2648, -0.3013, -0.2963, -0.3090, -0.2060, -0.3443, -0.3161, -0.2730,\n",
      "        -0.3868, -0.2896, -0.3272, -0.2118, -0.2835, -0.2692, -0.2939, -0.2182,\n",
      "        -0.2797, -0.2721, -0.1882, -0.3592, -0.2579, -0.2701, -0.3540, -0.4596,\n",
      "        -0.2629, -0.2788, -0.3492, -0.3603, -0.2716, -0.3488, -0.2716, -0.3258,\n",
      "        -0.3444, -0.2791, -0.2627, -0.2705, -0.2042, -0.2580, -0.1648, -0.2392,\n",
      "        -0.3101, -0.2479, -0.2717, -0.2574, -0.3061, -0.3664, -0.3465, -0.4245,\n",
      "        -0.2217, -0.3251, -0.2873, -0.3515, -0.3135, -0.2647, -0.4180, -0.2580,\n",
      "        -0.3778, -0.2709, -0.2371, -0.2693, -0.2734, -0.3207, -0.2425, -0.3828,\n",
      "        -0.3173, -0.2856, -0.2337, -0.3078, -0.1829, -0.2779, -0.3089, -0.3543,\n",
      "        -0.3167, -0.2950, -0.1015, -0.4058, -0.2448, -0.1528, -0.2801, -0.3152,\n",
      "        -0.2783, -0.2102, -0.3113, -0.2190, -0.2637, -0.1088, -0.2743, -0.1439,\n",
      "        -0.2998, -0.2703, -0.2665, -0.3873, -0.2004, -0.2914, -0.3378, -0.3053,\n",
      "        -0.3154, -0.2528, -0.3530, -0.2748, -0.2513, -0.3288, -0.2224, -0.2891,\n",
      "        -0.2573, -0.3120, -0.1725, -0.2479, -0.3111, -0.1921, -0.3139, -0.2054,\n",
      "        -0.2225, -0.4233, -0.3038, -0.3024, -0.3082, -0.2751, -0.2238, -0.2635,\n",
      "        -0.2286, -0.3047, -0.3154, -0.2448, -0.3704, -0.2979, -0.3988, -0.2494,\n",
      "        -0.3142, -0.2824, -0.2838, -0.3681, -0.1838, -0.2547, -0.3132, -0.2270,\n",
      "        -0.3749, -0.3189, -0.3063, -0.3214, -0.3353, -0.2597, -0.2356, -0.2825,\n",
      "        -0.3089, -0.1848, -0.3726, -0.2424, -0.2458, -0.2746, -0.3124, -0.3234,\n",
      "        -0.2245, -0.2481, -0.2030, -0.2533, -0.2738, -0.3700, -0.2214, -0.3937,\n",
      "        -0.2748, -0.2174, -0.2455, -0.2605, -0.2641, -0.2816, -0.2792, -0.2412,\n",
      "        -0.3654, -0.2991, -0.2753, -0.2679, -0.3134, -0.2765, -0.3176, -0.2621,\n",
      "        -0.4285, -0.2706, -0.2219, -0.2882, -0.2676, -0.4346, -0.2007, -0.2786,\n",
      "        -0.3178, -0.2547, -0.2809, -0.3347, -0.2870, -0.2956, -0.2264, -0.4821,\n",
      "        -0.2760, -0.2902, -0.3459, -0.2625, -0.2989, -0.2847, -0.2756, -0.2453,\n",
      "        -0.1895, -0.3156, -0.2073, -0.2254, -0.3555, -0.2597, -0.2759, -0.2888,\n",
      "        -0.2701, -0.3078, -0.2921, -0.3380, -0.3340, -0.2927, -0.2363, -0.2386,\n",
      "        -0.2770, -0.3205, -0.3443, -0.3745, -0.2709, -0.2635, -0.2527, -0.2841,\n",
      "        -0.2561, -0.3294, -0.2752, -0.1849, -0.2488, -0.2311, -0.3712, -0.2793,\n",
      "        -0.3499, -0.2513, -0.2582, -0.2544, -0.1964, -0.2909, -0.2058, -0.2302,\n",
      "        -0.2437, -0.2734, -0.2576, -0.2507, -0.2791, -0.2990, -0.2478, -0.2493,\n",
      "        -0.2205, -0.3412, -0.1456, -0.2653, -0.3025, -0.2237, -0.2385, -0.2448,\n",
      "        -0.2238, -0.3748, -0.2817, -0.2220, -0.2755, -0.3670, -0.2753, -0.2309,\n",
      "        -0.2302, -0.2456, -0.2341, -0.2862, -0.2272, -0.2411, -0.3351, -0.2707,\n",
      "        -0.2685, -0.2414, -0.3210, -0.2534, -0.2477, -0.2015, -0.3378, -0.2217],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Encoder2.weight Parameter containing:\n",
      "tensor([[ 0.0200,  0.0423,  0.0208,  ..., -0.1259, -0.0879,  0.1175],\n",
      "        [ 0.0125,  0.0227, -0.0237,  ..., -0.0761, -0.0549,  0.0368],\n",
      "        [-0.0261,  0.0089,  0.0344,  ..., -0.0815, -0.0520,  0.0535],\n",
      "        ...,\n",
      "        [-0.0227, -0.0446,  0.0473,  ...,  0.0373,  0.0220,  0.0109],\n",
      "        [-0.0153,  0.0379, -0.0243,  ..., -0.0871,  0.0020,  0.0594],\n",
      "        [-0.0226,  0.0160,  0.0045,  ...,  0.0135, -0.0835, -0.0419]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Encoder2.bias Parameter containing:\n",
      "tensor([-0.4657, -0.4660, -0.3886, -0.4677, -0.5336, -0.4965, -0.2057, -0.5277,\n",
      "        -0.3778, -0.2528, -0.4298, -0.4074, -0.4896, -0.1427, -0.4637, -0.2209,\n",
      "        -0.5543, -0.4108, -0.4896, -0.4101, -0.4016, -0.3938, -0.3264, -0.4346,\n",
      "        -0.3746, -0.3749, -0.3408, -0.5137, -0.2973, -0.4652, -0.3371, -0.5543,\n",
      "        -0.3108, -0.4166, -0.4316, -0.4849, -0.3625, -0.3658, -0.1957, -0.3417,\n",
      "        -0.4405, -0.2911, -0.3479, -0.5099, -0.3606, -0.4265, -0.4124, -0.4777,\n",
      "        -0.1638, -0.2419, -0.3505, -0.3537, -0.2696, -0.5733, -0.4262, -0.6567,\n",
      "        -0.5222, -0.3293, -0.3194, -0.3493, -0.4433, -0.4817, -0.4970, -0.5509,\n",
      "        -0.5072, -0.2450, -0.3920, -0.4915, -0.3109, -0.4045, -0.3764, -0.3480,\n",
      "        -0.3586, -0.4798, -0.3316, -0.5762, -0.3390, -0.2324, -0.4506, -0.4077,\n",
      "        -0.4168, -0.3523, -0.3871, -0.3315, -0.4120, -0.5693, -0.5574, -0.4822,\n",
      "        -0.3533, -0.3825, -0.5047, -0.4870, -0.3616, -0.3773, -0.3700, -0.4474,\n",
      "        -0.4518, -0.4075, -0.4436, -0.5470, -0.3511, -0.4956, -0.4814, -0.4172,\n",
      "        -0.3598, -0.4927, -0.5024, -0.2544, -0.4308, -0.4310, -0.4548, -0.5528,\n",
      "        -0.4441, -0.3979, -0.5427, -0.2635, -0.3585, -0.4404, -0.5596, -0.2928,\n",
      "        -0.4967, -0.4453, -0.3862, -0.3865, -0.4491, -0.3378, -0.4297, -0.4743,\n",
      "        -0.3790, -0.3833, -0.4986, -0.4916, -0.4519, -0.3621, -0.4993, -0.2924,\n",
      "        -0.5141, -0.4249, -0.5345, -0.2566, -0.4632, -0.4946, -0.4985, -0.4977,\n",
      "        -0.3868, -0.4437, -0.2354, -0.4090, -0.2512, -0.3803, -0.3284, -0.2832,\n",
      "        -0.3848, -0.2179, -0.3352, -0.3702, -0.3975, -0.4748, -0.2854, -0.3557,\n",
      "        -0.3877, -0.4736, -0.2988, -0.4973, -0.3997, -0.4266, -0.4324, -0.4218,\n",
      "        -0.4996, -0.4682, -0.1848, -0.4375, -0.3895, -0.3715, -0.4734, -0.3627,\n",
      "        -0.3532, -0.4111, -0.4042, -0.3270, -0.3450, -0.3548, -0.4676, -0.4343,\n",
      "        -0.4063, -0.5602, -0.4533, -0.4924, -0.3657, -0.3993, -0.2625, -0.1929,\n",
      "        -0.2943, -0.4846, -0.3525, -0.3139, -0.3162, -0.4013, -0.3858, -0.3562,\n",
      "        -0.4845, -0.4005, -0.4134, -0.3716, -0.4068, -0.3544, -0.2895, -0.4940,\n",
      "        -0.4815, -0.5668, -0.4824, -0.5052, -0.3597, -0.4894, -0.3560, -0.4047,\n",
      "        -0.3621, -0.4124, -0.4140, -0.4161, -0.5279, -0.4773, -0.4355, -0.3816,\n",
      "        -0.4478, -0.3141, -0.4108, -0.2873, -0.4008, -0.4736, -0.4760, -0.3985,\n",
      "        -0.4614, -0.3891, -0.2596, -0.3968, -0.3715, -0.2214, -0.3257, -0.2781,\n",
      "        -0.5303, -0.3580, -0.6299, -0.4192, -0.4482, -0.5192, -0.4000, -0.4900,\n",
      "        -0.4640, -0.4749, -0.4909, -0.4419, -0.4684, -0.3089, -0.3865, -0.1080,\n",
      "        -0.3116, -0.5191, -0.3792, -0.4160, -0.4448, -0.4369, -0.3834, -0.3138,\n",
      "        -0.4685, -0.4179, -0.3246, -0.4236, -0.4647, -0.5087, -0.4053, -0.4386,\n",
      "        -0.2420, -0.1494, -0.5081, -0.3783, -0.3025, -0.4828, -0.5885, -0.3552,\n",
      "        -0.2837, -0.3152, -0.0609, -0.3070, -0.4041, -0.3950, -0.3559, -0.3122,\n",
      "        -0.5563, -0.3823, -0.3002, -0.5347, -0.4113, -0.3264, -0.4469, -0.4855,\n",
      "        -0.4096, -0.3759, -0.4717, -0.3985, -0.4380, -0.4758, -0.5476, -0.3287,\n",
      "        -0.4148, -0.3910, -0.3397, -0.5430, -0.4006, -0.4457, -0.4350, -0.4257,\n",
      "        -0.5790, -0.4533, -0.3174, -0.2328, -0.4043, -0.3718, -0.2789, -0.3619,\n",
      "        -0.3686, -0.2845, -0.3619, -0.4240, -0.4420, -0.4834, -0.3334, -0.3576,\n",
      "        -0.4297, -0.4723, -0.4604, -0.5043, -0.4728, -0.2623, -0.3762, -0.4755,\n",
      "        -0.4003, -0.3670, -0.3885, -0.3228, -0.4661, -0.4989, -0.3280, -0.4513,\n",
      "        -0.3835, -0.5264, -0.3169, -0.2597, -0.4450, -0.4662, -0.4570, -0.0664,\n",
      "        -0.5292, -0.4871, -0.4208, -0.4315, -0.4818, -0.5224, -0.4239, -0.4064,\n",
      "        -0.4483, -0.3269, -0.3353, -0.4020, -0.4661, -0.3786, -0.5754, -0.4153,\n",
      "        -0.3560, -0.4726, -0.5006, -0.3396, -0.2687, -0.5098, -0.4326, -0.5317,\n",
      "        -0.4505, -0.4548, -0.5121, -0.4795, -0.4499, -0.3033, -0.4627, -0.2938,\n",
      "        -0.3230, -0.3208, -0.4302, -0.4238, -0.3244, -0.4346, -0.4997, -0.5303,\n",
      "        -0.3980, -0.4517, -0.3584, -0.4102, -0.3306, -0.3805, -0.3248, -0.4242,\n",
      "        -0.4505, -0.2962, -0.4424, -0.4511, -0.3482, -0.5288, -0.5051, -0.3899,\n",
      "        -0.2979, -0.4952, -0.4372, -0.2274, -0.4347, -0.4565, -0.5522, -0.5103,\n",
      "        -0.3234, -0.5403, -0.3865, -0.4386, -0.3898, -0.5241, -0.4937, -0.0740,\n",
      "        -0.4733, -0.3011, -0.4514, -0.4768, -0.3235, -0.5214, -0.3109, -0.3749,\n",
      "        -0.5300, -0.4160, -0.4940, -0.3693, -0.4351, -0.3030, -0.5215, -0.5125,\n",
      "        -0.1700, -0.5078, -0.4062, -0.4263, -0.3888, -0.3985, -0.4720, -0.4996,\n",
      "        -0.4135, -0.4584, -0.4872, -0.3822, -0.4052, -0.4461, -0.4577, -0.3276,\n",
      "        -0.4472, -0.4052, -0.4403, -0.3813, -0.4314, -0.3042, -0.4523, -0.4499,\n",
      "        -0.2836, -0.5099, -0.4189, -0.3709, -0.3735, -0.3364, -0.2808, -0.3795,\n",
      "        -0.4300, -0.5054, -0.4033, -0.3660, -0.4179, -0.4808, -0.4924, -0.4967,\n",
      "        -0.3779, -0.4269, -0.3742, -0.4034, -0.4590, -0.3827, -0.3790, -0.4835,\n",
      "        -0.4901, -0.4683, -0.3562, -0.6567, -0.5062, -0.5385, -0.5189, -0.4484,\n",
      "        -0.3567, -0.2954, -0.2724, -0.4423, -0.3375, -0.4119, -0.4776, -0.4401,\n",
      "        -0.4234, -0.5510, -0.4813, -0.4206, -0.2522, -0.5231, -0.4513, -0.5105],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "DenseRes0.FC0.weight Parameter containing:\n",
      "tensor([[-0.0775, -0.0064, -0.0510,  ..., -0.0043, -0.0301, -0.0015],\n",
      "        [ 0.0517, -0.0349,  0.0411,  ..., -0.0302,  0.0051,  0.0243],\n",
      "        [ 0.0110,  0.0172, -0.0879,  ...,  0.0141,  0.0051, -0.0188],\n",
      "        ...,\n",
      "        [ 0.0063, -0.0092, -0.0204,  ..., -0.1305,  0.0003, -0.0358],\n",
      "        [ 0.0056, -0.0841,  0.0102,  ...,  0.0606, -0.1223, -0.0771],\n",
      "        [ 0.0249,  0.0041, -0.0096,  ...,  0.0202, -0.1051, -0.1577]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "DenseRes0.FC0.bias Parameter containing:\n",
      "tensor([-0.0224, -0.0647, -0.0145,  ..., -0.0282,  0.0014,  0.0359],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "DenseRes0.FC1.weight Parameter containing:\n",
      "tensor([[-0.0737, -0.0338,  0.0311,  ..., -0.0242, -0.0117,  0.0082],\n",
      "        [-0.0054, -0.0631,  0.0070,  ...,  0.0044,  0.0360, -0.0319],\n",
      "        [-0.0416, -0.0213, -0.0777,  ..., -0.0098,  0.0285, -0.0088],\n",
      "        ...,\n",
      "        [-0.0117, -0.0851, -0.0075,  ..., -0.0698, -0.0015, -0.0039],\n",
      "        [ 0.0165, -0.0517, -0.0157,  ..., -0.0226, -0.1172,  0.0326],\n",
      "        [-0.0314,  0.0082, -0.0248,  ..., -0.0184,  0.0091, -0.0266]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "DenseRes0.FC1.bias Parameter containing:\n",
      "tensor([-0.0423, -0.0612, -0.0310,  ..., -0.1044, -0.0595, -0.1060],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "DenseRes1.FC0.weight Parameter containing:\n",
      "tensor([[-0.0264, -0.0096, -0.0427,  ...,  0.0178,  0.0038,  0.0532],\n",
      "        [ 0.0268, -0.0274,  0.0062,  ...,  0.0245,  0.0287, -0.0350],\n",
      "        [-0.0350, -0.0038, -0.0493,  ..., -0.0351,  0.0263,  0.0284],\n",
      "        ...,\n",
      "        [ 0.0236, -0.0454,  0.0349,  ..., -0.0291, -0.0442,  0.0035],\n",
      "        [-0.0005,  0.0202, -0.0289,  ..., -0.0240, -0.0661, -0.0209],\n",
      "        [-0.0266, -0.0543, -0.0307,  ..., -0.0123, -0.0232, -0.0683]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "DenseRes1.FC0.bias Parameter containing:\n",
      "tensor([ 0.0944,  0.0378, -0.0911,  ...,  0.0016, -0.0856, -0.1828],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "DenseRes1.FC1.weight Parameter containing:\n",
      "tensor([[-0.1388,  0.0460,  0.0260,  ..., -0.0190,  0.0269,  0.0267],\n",
      "        [ 0.0064, -0.0743,  0.0202,  ...,  0.0291,  0.0511,  0.0196],\n",
      "        [ 0.0369,  0.0014, -0.0146,  ...,  0.0172,  0.0224,  0.0016],\n",
      "        ...,\n",
      "        [ 0.0277, -0.0417,  0.0457,  ..., -0.0415,  0.0120, -0.0253],\n",
      "        [-0.0237,  0.0103,  0.0053,  ..., -0.0260, -0.1017,  0.0271],\n",
      "        [ 0.0041,  0.0202, -0.0264,  ...,  0.0091, -0.0036, -0.1691]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "DenseRes1.FC1.bias Parameter containing:\n",
      "tensor([-0.1066, -0.1076, -0.0680,  ...,  0.0069,  0.0385,  0.1092],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Decoder.weight Parameter containing:\n",
      "tensor([[ 3.0861e-04,  1.1433e-04, -3.4010e-04,  ..., -1.1636e-04,\n",
      "          3.1003e-04, -2.4568e-04],\n",
      "        [ 2.9357e-04,  1.1943e-04, -3.0039e-04,  ..., -1.1222e-04,\n",
      "          2.8861e-04, -2.4852e-04],\n",
      "        [ 2.9777e-04,  1.1821e-04, -3.1073e-04,  ..., -1.1300e-04,\n",
      "          2.9425e-04, -2.4835e-04],\n",
      "        ...,\n",
      "        [-1.0075e-02,  1.5461e-02,  1.8501e-03,  ...,  2.8796e-03,\n",
      "          6.4911e-03, -6.0163e-03],\n",
      "        [ 8.9461e-03,  4.2349e-03,  1.8365e-03,  ...,  5.4857e-03,\n",
      "          5.1442e-03, -8.6557e-03],\n",
      "        [ 5.4899e-03,  3.7188e-03,  1.2551e-03,  ...,  5.4487e-03,\n",
      "          1.2318e-03,  1.6582e-05]], device='cuda:0', requires_grad=True)\n",
      "Decoder.bias Parameter containing:\n",
      "tensor([ 9.6603e-04,  9.7491e-04,  9.7371e-04,  9.7539e-04,  9.7093e-04,\n",
      "         9.7559e-04, -1.1627e-02, -6.4976e-03, -1.0413e-02,  1.3800e-03,\n",
      "        -7.0803e-03, -1.7432e-02,  5.6770e-03,  4.9800e-03, -1.5724e-02,\n",
      "         9.7479e-04, -9.7592e-04, -9.7483e-04, -1.5267e-02,  6.0294e-04,\n",
      "         9.4852e-03, -1.3901e-02,  4.1038e-04,  2.6089e-03, -9.7226e-04,\n",
      "         9.7557e-04,  9.6355e-04, -1.1959e-02, -2.4543e-03,  3.1645e-03,\n",
      "        -2.7604e-03, -4.7162e-04, -7.6698e-03,  9.8776e-04,  9.7278e-04,\n",
      "        -9.7402e-04, -1.3682e-02, -5.3137e-03,  4.7898e-04, -5.0328e-03,\n",
      "        -4.6622e-03, -7.9847e-03,  9.6916e-04, -9.7312e-04,  9.7553e-04,\n",
      "        -1.0558e-02, -1.0322e-03, -3.8704e-03, -5.4300e-03, -1.4223e-04,\n",
      "        -7.8234e-03,  2.7315e-03,  1.5021e-02, -1.5554e-02, -9.4068e-03,\n",
      "         1.0412e-03, -3.8226e-03, -1.1028e-03,  5.1591e-04, -1.3455e-02,\n",
      "        -2.5723e-03, -3.9613e-03, -1.3827e-02, -2.4042e-03, -1.2752e-03,\n",
      "        -1.4968e-02,  9.7393e-04,  9.7391e-04,  9.7559e-04,  9.7294e-04,\n",
      "        -6.7830e-04,  9.7742e-04,  9.7529e-04,  9.7679e-04, -1.0259e-03,\n",
      "         9.7731e-04, -9.7514e-04, -9.8959e-04, -9.7099e-04,  9.7295e-04,\n",
      "         9.4509e-04,  9.7305e-04, -9.7589e-04, -9.7036e-04,  9.6391e-04,\n",
      "         9.7361e-04,  9.7910e-04,  9.6695e-04,  9.6712e-04,  9.7256e-04,\n",
      "        -9.7522e-04,  7.3511e-04, -9.8999e-04,  9.7643e-04,  9.8909e-04,\n",
      "        -9.6605e-04, -9.7052e-04, -9.6615e-04,  9.7545e-04, -3.8939e-03,\n",
      "         8.7489e-03, -8.4047e-04, -7.7556e-03,  1.3161e-02, -7.4601e-03,\n",
      "        -1.3612e-02,  1.3061e-02,  2.2883e-02, -1.4944e-02,  1.4211e-02,\n",
      "        -2.6326e-03,  1.0056e-02, -1.2167e-02,  1.3129e-02, -1.2800e-02,\n",
      "         1.3146e-02, -1.4862e-03,  2.3418e-02, -1.7318e-02,  1.6237e-02,\n",
      "        -9.8280e-03, -1.4988e-02, -1.4039e-02,  2.3681e-03, -7.7658e-03,\n",
      "        -8.2768e-03,  1.8261e-03, -6.6253e-03, -9.3640e-03, -4.5223e-03,\n",
      "        -4.4037e-04,  1.1923e-02,  3.3373e-03, -5.4871e-03, -8.4264e-03,\n",
      "        -1.8458e-02, -6.7889e-03,  3.3110e-03, -4.0412e-03, -3.6567e-03,\n",
      "        -1.2408e-02,  4.1589e-03, -6.2174e-03, -9.1203e-03,  4.2055e-03,\n",
      "        -5.5012e-03, -9.3011e-03, -1.7515e-02, -8.1383e-03, -1.0874e-03,\n",
      "         9.1161e-03, -3.1646e-03,  7.6547e-04, -2.4463e-02, -9.7476e-03,\n",
      "        -4.1267e-03, -1.4997e-03,  2.9522e-04, -1.3327e-02,  5.3911e-03,\n",
      "        -6.4508e-03, -1.8259e-02,  5.5589e-03, -4.7879e-03, -1.7375e-02,\n",
      "        -1.5943e-02, -7.3264e-03,  1.4223e-03,  5.7872e-03, -4.5253e-03,\n",
      "        -1.7090e-02, -2.3998e-02, -8.7034e-03, -4.2154e-04,  3.5821e-03,\n",
      "         6.9185e-03, -8.0447e-03,  4.0517e-03,  2.9190e-04, -2.3095e-02,\n",
      "         5.7134e-03,  1.3855e-03, -1.6857e-02, -1.0624e-02, -8.0072e-03,\n",
      "         1.8846e-03,  4.1223e-03,  1.1915e-03, -2.2278e-02, -2.0170e-02,\n",
      "        -1.1027e-02,  1.5862e-04,  9.7374e-04, -1.8136e-03,  6.8011e-03,\n",
      "        -1.2078e-02, -3.2302e-02, -1.4836e-02, -2.3316e-02, -2.7173e-02,\n",
      "        -5.8858e-03, -2.1108e-02, -2.8265e-02, -4.0770e-03, -7.4346e-03,\n",
      "        -2.0806e-02, -2.6210e-03,  8.9441e-05, -7.8058e-03, -2.7251e-02,\n",
      "        -2.4184e-02, -1.9776e-02, -1.5242e-02, -8.6231e-03], device='cuda:0',\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for name, param in model.named_parameters():\n",
    "        print(name, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_mm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_105243/2083434039.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbindata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Xstd\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbindata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Ystd\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m192\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m192\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m528\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m528\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m# predicted, actual = classes[pred[0].argmax(0)], classes[y]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Predicted: {pred}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_105243/3245855241.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pose, traj, sensor)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mpose_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEncoder0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mtraj_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEncoder1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0msensor_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEncoder2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/render-hand/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/render-hand/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_mm)"
     ]
    }
   ],
   "source": [
    "# test data\n",
    "# model.eval()\n",
    "# 2350 = 192 + 336 + 1822\n",
    "\n",
    "# x, y = torch.from_numpy(bindata[\"Xstd\"]), torch.from_numpy(bindata[\"Ystd\"])\n",
    "# with torch.no_grad():\n",
    "#     pred = model.forward(x[:192], x[192:528], x[528:])\n",
    "#     # predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "#     print(f'Predicted: {pred}')\n",
    "#     print(f\"Actual: {y}\")\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2350,) [-0.61872  0.44557  0.17121 -0.65208  0.34153  0.36247  0.28213 -0.08111\n",
      " -0.28643 -0.33598  0.17554 -0.23346 -0.46493  0.07751 -0.10986 -0.52472\n",
      "  0.10809  0.13183 -0.62187  0.22497]\n",
      "(214,) [0.92892 0.77443 0.53388 0.6854  0.53704 0.5237  0.80279 0.66747 0.57714\n",
      " 0.83595 0.76592 0.66795 0.87527 0.8128  0.73279 0.63066 0.63877 0.80745\n",
      " 0.79434 0.76837]\n"
     ]
    }
   ],
   "source": [
    "Xmean = np.fromfile(\"Xmean.bin\", dtype=np.float32)\n",
    "Ymean = np.fromfile(\"Ymean.bin\", dtype=np.float32)\n",
    "print(Xmean.shape, Xmean[-20:])\n",
    "print(Ymean.shape, Ymean[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([ 0.0000,  0.0000,  0.0000, -0.2251,  0.1177, -0.0623, -0.4226,  0.2596,\n",
      "        -0.1927, -0.7172,  0.3679, -0.3507, -1.0464,  0.4562, -0.4728, -1.0769])\n",
      "Actual: tensor([ 0.0000,  0.0000,  0.0000, -0.2251,  0.1177, -0.0623, -0.4226,  0.2596,\n",
      "        -0.1927, -0.7172,  0.3679, -0.3507, -1.0464,  0.4563, -0.4728, -1.0769],\n",
      "       device='cuda:0')\n",
      "Predicted: tensor([ 0.0005,  0.0005,  0.0005,  0.0005,  0.0005,  0.0005, -0.1027,  0.0645,\n",
      "        -0.0620, -0.3811,  0.1370,  0.0357, -0.4754, -0.0834,  0.0494,  0.0005],\n",
      "       device='cuda:0')\n",
      "Loss: 0.21744012832641602\n"
     ]
    }
   ],
   "source": [
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn = nn.MSELoss()\n",
    "model.to(device)\n",
    "\n",
    "def test(bindata, model):\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        x, y = torch.from_numpy(bindata[\"Xmean\"]), torch.from_numpy(bindata[\"Ymean\"])\n",
    "        pose = x[:192].to(device).unsqueeze(0)\n",
    "        traj = x[192:528].to(device).unsqueeze(0)\n",
    "        sensor = x[528:].to(device).unsqueeze(0)\n",
    "        y = y.to(device).unsqueeze(0)\n",
    "        pred = model(pose, traj, sensor)\n",
    "        test_loss = loss_fn(pred, y)\n",
    "    \n",
    "    torch.set_printoptions(threshold=10_000)\n",
    "    print(f\"Input: {x[:16]}\")\n",
    "    print(f\"Actual: {y.squeeze(0)[:16]}\")\n",
    "    print(f'Predicted: {pred.squeeze(0)[:16]}')\n",
    "    print(f\"Loss: {test_loss}\")\n",
    "\n",
    "test(bindata, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
